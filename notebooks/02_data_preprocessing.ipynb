{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b37f13c5",
      "metadata": {
        "id": "b37f13c5"
      },
      "source": [
        "# Przetwarzanie Danych - Przygotowanie do Modelowania\n",
        "\n",
        "Ten notebook zawiera wszystkie kroki przetwarzania danych niezbędne do trenowania modeli prognozowania popytu.\n",
        "\n",
        "## Cele przetwarzania:\n",
        "- Czyszczenie i filtrowanie danych\n",
        "- Tworzenie cech czasowych\n",
        "- Agregacja danych do poziomia dziennego/tygodniowego\n",
        "- Przygotowanie sekwencji dla modeli LSTM\n",
        "- Podział na zbiory treningowe/walidacyjne/testowe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cfbb3fbc",
      "metadata": {
        "id": "cfbb3fbc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "DATA_DIR = Path('/content/data')\n",
        "OUT_DIR  = Path('/content/data_processed')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LAG_LIST      = [7, 14, 28]\n",
        "ROLL_WINDOWS  = [7, 28]\n",
        "SEQ_LENGTH    = 60\n",
        "PRED_LENGTH   = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f11d8d",
      "metadata": {
        "id": "b6f11d8d"
      },
      "source": [
        "## 1. Load raw CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ffd97383",
      "metadata": {
        "id": "ffd97383"
      },
      "outputs": [],
      "source": [
        "def load_raw(path: Path = DATA_DIR) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Read every CSV we need and parse dates.\"\"\"\n",
        "    files = {f.stem: f for f in path.glob('*.csv')}\n",
        "    required = [\"train\", \"test\", \"stores\", \"oil\", \"holidays_events\", \"transactions\"]\n",
        "    missing  = [k for k in required if k not in files]\n",
        "    if missing:\n",
        "        raise FileNotFoundError(f\"Missing files: {missing}\")\n",
        "\n",
        "    df = {\n",
        "        'train':  pd.read_csv(files['train'],  parse_dates=['date']),\n",
        "        'test':   pd.read_csv(files['test'],   parse_dates=['date']),\n",
        "        'stores': pd.read_csv(files['stores']),\n",
        "        'oil':    pd.read_csv(files['oil'],    parse_dates=['date']),\n",
        "        'holidays':      pd.read_csv(files['holidays_events'], parse_dates=['date']),\n",
        "        'transactions':  pd.read_csv(files['transactions'],   parse_dates=['date']),\n",
        "    }\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18eb6068",
      "metadata": {
        "id": "18eb6068"
      },
      "source": [
        "## 2. Basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a346532",
      "metadata": {
        "id": "0a346532"
      },
      "outputs": [],
      "source": [
        "def basic_clean(train: pd.DataFrame) -> pd.DataFrame:\n",
        "    before = len(train)\n",
        "    train = train.drop_duplicates(subset=['date', 'store_nbr', 'family'])\n",
        "    print(f\"Dropped {before - len(train):,} duplicate rows\")\n",
        "\n",
        "    upper = train['sales'].quantile(0.999)\n",
        "    train['sales'] = train['sales'].clip(upper=upper)\n",
        "    return train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7738b5ef",
      "metadata": {
        "id": "7738b5ef"
      },
      "source": [
        "## 3. Calendar features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a3fa5964",
      "metadata": {
        "id": "a3fa5964"
      },
      "outputs": [],
      "source": [
        "def add_calendar(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df['date']\n",
        "    df['year']      = d.dt.year.astype('int16')\n",
        "    df['month']     = d.dt.month.astype('int8')\n",
        "    df['dow']       = d.dt.weekday.astype('int8')\n",
        "    df['weekofyr']  = d.dt.isocalendar().week.astype('int8')\n",
        "\n",
        "    df['dow_sin']   = np.sin(2 * np.pi * df['dow']   / 7)\n",
        "    df['dow_cos']   = np.cos(2 * np.pi * df['dow']   / 7)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b584da87",
      "metadata": {
        "id": "b584da87"
      },
      "source": [
        "## 4. Merge external tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8cace1c7",
      "metadata": {
        "id": "8cace1c7"
      },
      "outputs": [],
      "source": [
        "def merge_external(train: pd.DataFrame, raw: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
        "    oil = raw['oil'].set_index('date')\n",
        "    oil = oil.reindex(pd.date_range('2013-01-01', '2017-08-31', freq='D'))\n",
        "    oil['dcoilwtico'] = oil['dcoilwtico'].interpolate().ffill().bfill()\n",
        "    oil['oil_ma30']   = oil['dcoilwtico'].rolling(30).mean()\n",
        "    oil['oil_pct_7']  = oil['dcoilwtico'].pct_change(7)\n",
        "    oil = oil.reset_index().rename(columns={'index': 'date'})\n",
        "\n",
        "    hol = raw['holidays']\n",
        "    nat_hol = hol[(hol['locale'] == 'National') & (hol['transferred'] == False)]\n",
        "    nat_flag = pd.DataFrame({'date': nat_hol['date'].dt.normalize(), 'is_natl_holiday': 1})\n",
        "\n",
        "    tx = (raw['transactions']\n",
        "          .groupby('date')['transactions']\n",
        "          .sum()\n",
        "          .rolling(7).mean()\n",
        "          .reset_index())\n",
        "\n",
        "    stores = raw['stores']\n",
        "\n",
        "    df = (train\n",
        "          .merge(oil, on='date', how='left')\n",
        "          .merge(nat_flag, on='date', how='left')\n",
        "          .merge(tx, on='date', how='left')\n",
        "          .merge(stores, on='store_nbr', how='left'))\n",
        "\n",
        "    df['is_natl_holiday'] = df['is_natl_holiday'].fillna(0).astype('int8')\n",
        "    df['transactions']    = df['transactions'].fillna(method='ffill').fillna(0)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea38bc21",
      "metadata": {
        "id": "ea38bc21"
      },
      "source": [
        "## 5. Lag & rolling statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "78fc7a7c",
      "metadata": {
        "id": "78fc7a7c"
      },
      "outputs": [],
      "source": [
        "def add_lags(df: pd.DataFrame, group_cols: List[str]) -> pd.DataFrame:\n",
        "    g = df.sort_values('date').groupby(group_cols)\n",
        "    for lag in LAG_LIST:\n",
        "        df[f'lag_{lag}'] = g['sales'].shift(lag)\n",
        "    for win in ROLL_WINDOWS:\n",
        "        df[f'roll_mean_{win}'] = g['sales'].shift(1).rolling(win).mean()\n",
        "        df[f'roll_std_{win}']  = g['sales'].shift(1).rolling(win).std()\n",
        "    df['is_zero'] = (df['sales'] == 0).astype('int8')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode_categorical(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, LabelEncoder]]:\n",
        "    categorical_cols = ['family', 'city', 'state', 'type']\n",
        "    encoders = {}\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            print(f\"  Encoding {col}: {df[col].nunique()} unique values\")\n",
        "            encoder = LabelEncoder()\n",
        "            df[col] = encoder.fit_transform(df[col].astype(str))\n",
        "            encoders[col] = encoder\n",
        "        else:\n",
        "            print(f\"  Warning: Column {col} not found in dataframe\")\n",
        "\n",
        "    joblib.dump(encoders, OUT_DIR / 'categorical_encoders.pkl')\n",
        "    print(f\"Saved {len(encoders)} categorical encoders\")\n",
        "\n",
        "    return df, encoders"
      ],
      "metadata": {
        "id": "FGmROI5dS3YB"
      },
      "id": "FGmROI5dS3YB",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f8121de6",
      "metadata": {
        "id": "f8121de6"
      },
      "source": [
        "## 6. Scaling numeric columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a4839cf6",
      "metadata": {
        "id": "a4839cf6"
      },
      "outputs": [],
      "source": [
        "def scale_numeric(df: pd.DataFrame) -> Dict[str, StandardScaler]:\n",
        "    num_cols = (\n",
        "        df.select_dtypes(include=['float64', 'float32'])\n",
        "          .columns.difference(['sales'])\n",
        "          .tolist()\n",
        "    )\n",
        "\n",
        "    scalers = {}\n",
        "    for col in num_cols:\n",
        "        sc = StandardScaler()\n",
        "        df[col] = sc.fit_transform(df[[col]])\n",
        "        scalers[col] = sc\n",
        "    joblib.dump(scalers, OUT_DIR / 'scalers.pkl')\n",
        "    return scalers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9540e764",
      "metadata": {
        "id": "9540e764"
      },
      "source": [
        "## 7. Chronological split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9f920f74",
      "metadata": {
        "id": "9f920f74"
      },
      "outputs": [],
      "source": [
        "def chrono_split(df: pd.DataFrame,\n",
        "                 train_end: str = '2016-12-31',\n",
        "                 val_end: str   = '2017-06-30') -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    train = df[df['date'] <= train_end]\n",
        "    val   = df[(df['date'] > train_end) & (df['date'] <= val_end)]\n",
        "    test  = df[df['date'] > val_end]\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecc3203",
      "metadata": {
        "id": "cecc3203"
      },
      "source": [
        "## 8. Sequence builder (optional for LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7ab048e8",
      "metadata": {
        "id": "7ab048e8"
      },
      "outputs": [],
      "source": [
        "def build_sequences(df: pd.DataFrame, group_cols: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    X_list, y_list = [], []\n",
        "    feature_cols = df.columns.difference(['sales', 'date']).tolist()\n",
        "\n",
        "    for _, grp in df.groupby(group_cols):\n",
        "        grp = grp.sort_values('date')\n",
        "        Xv = grp[feature_cols].values.astype(np.float32)\n",
        "        yv = grp['sales'].values.astype(np.float32)\n",
        "        for i in range(SEQ_LENGTH, len(grp) - PRED_LENGTH + 1):\n",
        "            X_list.append(Xv[i - SEQ_LENGTH:i])\n",
        "            y_list.append(yv[i:i + PRED_LENGTH])\n",
        "\n",
        "    return np.stack(X_list), np.stack(y_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096d8ae2",
      "metadata": {
        "id": "096d8ae2"
      },
      "source": [
        "## 9. Run the full pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "25471db3",
      "metadata": {
        "id": "25471db3"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(build_seq: bool = False):\n",
        "    raw = load_raw()\n",
        "    train = basic_clean(raw['train'])\n",
        "\n",
        "    train = add_calendar(train)\n",
        "    train = merge_external(train, raw)\n",
        "\n",
        "    train = add_lags(train, ['store_nbr', 'family'])\n",
        "\n",
        "    train.dropna(inplace=True)\n",
        "\n",
        "    train, categorical_encoders = encode_categorical(train)\n",
        "\n",
        "    scale_numeric(train)\n",
        "\n",
        "    train_df, val_df, test_df = chrono_split(train)\n",
        "\n",
        "    if build_seq:\n",
        "        Xtr, ytr = build_sequences(train_df, ['store_nbr', 'family'])\n",
        "        Xval, yval = build_sequences(val_df,   ['store_nbr', 'family'])\n",
        "        Xte, yte  = build_sequences(test_df,  ['store_nbr', 'family'])\n",
        "        np.savez_compressed(OUT_DIR / 'lstm_data.npz',\n",
        "                            X_train=Xtr, y_train=ytr,\n",
        "                            X_val=Xval, y_val=yval,\n",
        "                            X_test=Xte, y_test=yte)\n",
        "    else:\n",
        "        train_df.to_parquet(OUT_DIR / 'train.parquet')\n",
        "        val_df.to_parquet(OUT_DIR / 'val.parquet')\n",
        "        test_df.to_parquet(OUT_DIR / 'test.parquet')\n",
        "\n",
        "    print('Pipeline complete. Files saved to', OUT_DIR.resolve())\n",
        "    print(f'Encoded categorical columns: {list(categorical_encoders.keys())}')\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "46789943",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46789943",
        "outputId": "9f2c8c15-93eb-4a7a-e2ae-7b1f068a6ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 0 duplicate rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1337864506.py:28: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['transactions']    = df['transactions'].fillna(method='ffill').fillna(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Encoding family: 33 unique values\n",
            "  Encoding city: 22 unique values\n",
            "  Encoding state: 16 unique values\n",
            "  Encoding type: 5 unique values\n",
            "Saved 4 categorical encoders\n",
            "Pipeline complete. Files saved to /content/data_processed\n",
            "Encoded categorical columns: ['family', 'city', 'state', 'type']\n"
          ]
        }
      ],
      "source": [
        "train_df, val_df, test_df = run_pipeline(build_seq=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing Pipeline – key steps\n",
        "\n",
        "- **Load raw CSVs** – read `train`, `test`, `stores`, `oil`, `holidays_events`, `transactions`; parse the `date` column.\n",
        "\n",
        "- **Basic cleaning** – drop duplicate `(date, store_nbr, family)` rows and cap the top 0.1 % of `sales` values to reduce extreme spikes.\n",
        "\n",
        "- **Calendar features** – add `year`, `month`, `dow`, `weekofyr` plus cyclic encodings `dow_sin`, `dow_cos`, `month_sin`, `month_cos` to capture weekly and annual seasonality.\n",
        "\n",
        "- **Merge external tables** – join  \n",
        "  * interpolated WTI price (`dcoilwtico`) + 30-day moving average + 7-day % change,  \n",
        "  * national-holiday flag,  \n",
        "  * 7-day-smoothed `transactions`,  \n",
        "  * store metadata (`city`, `state`, `type`, `cluster`).\n",
        "\n",
        "- **Lag & rolling statistics** – per `(store_nbr, family)` create `lag_7/14/28`, rolling mean & std for 7/28-day windows, and an `is_zero` flag.\n",
        "\n",
        "- **Drop warm-up NaNs** – remove rows that lack full lag/rolling history.\n",
        "\n",
        "- **Scale numeric features** – apply `StandardScaler` to all float columns; save fitted scalers to `scalers.pkl`.\n",
        "\n",
        "- **Chronological split** – slice into  \n",
        "  * **train** ≤ 2016-12-31,  \n",
        "  * **validation** 2017-01-01 → 2017-06-30,  \n",
        "  * **test** ≥ 2017-07-01,  \n",
        "  ensuring no future leakage.\n",
        "\n",
        "- **Save processed data** – by default write three Parquet files (`train/val/test.parquet`); if `build_seq=True`, also export 60-day LSTM tensors in `lstm_data.npz`.\n",
        "\n",
        "- **Return ready datasets** – the function returns `train_df`, `val_df`, `test_df` in memory, giving clean, feature-rich, leak-free inputs for baselines or neural nets.\n"
      ],
      "metadata": {
        "id": "9xW9TCarz81Q"
      },
      "id": "9xW9TCarz81Q"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5C4QJ6sSz9UH"
      },
      "id": "5C4QJ6sSz9UH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
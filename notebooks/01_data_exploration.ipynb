{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Colab url\n",
    "https://colab.research.google.com/drive/1R6nEezCr-MayFCobKtLLtqjCWQ3npObu?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Store Sales Data Exploration\n",
    "\n",
    "Exploratory Data Analysis for the Store Sales Time Series Forecasting dataset from Kaggle.\n",
    "\n",
    "## Dataset Overview:\n",
    "- **train.csv**: Sales data with store, item family, and dates\n",
    "- **stores.csv**: Store metadata (city, state, type, cluster)\n",
    "- **oil.csv**: Daily oil prices (economic indicator)\n",
    "- **holidays_events.csv**: Holiday and event information\n",
    "- **transactions.csv**: Number of transactions per store/date\n",
    "\n",
    "## Analysis Goals:\n",
    "- Understand data structure and quality\n",
    "- Identify sales patterns and trends\n",
    "- Explore seasonality and holidays impact\n",
    "- Analyze store and product performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = Path('../data/raw')\n",
    "\n",
    "print(f\"Data files available: {list(DATA_PATH.glob('*.csv'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(DATA_PATH / 'train.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv(DATA_PATH / 'stores.csv')\n",
    "oil = pd.read_csv(DATA_PATH / 'oil.csv', parse_dates=['date'])\n",
    "holidays = pd.read_csv(DATA_PATH / 'holidays_events.csv', parse_dates=['date'])\n",
    "transactions = pd.read_csv(DATA_PATH / 'transactions.csv', parse_dates=['date'])\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "for name, df in [('train', train), ('stores', stores), ('oil', oil), ('holidays', holidays), ('transactions', transactions)]:\n",
    "    print(f\"{name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data sample:\")\n",
    "print(train.head(50))\n",
    "print(\"\\nData types:\")\n",
    "print(train.dtypes)\n",
    "print(f\"\\nDate range: {train.date.min()} to {train.date.max()}\")\n",
    "print(f\"Unique stores: {train.store_nbr.nunique()}\")\n",
    "print(f\"Product families: {train.family.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"Missing values:\")\n",
    "for name, df in [('train', train), ('stores', stores), ('oil', oil), ('holidays', holidays), ('transactions', transactions)]:\n",
    "    missing = df.isnull().sum().sum()\n",
    "    print(f\"{name}: {missing} ({missing/df.size*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTrain dataset statistics:\")\n",
    "print(train.describe())\n",
    "\n",
    "neg_sales = (train.sales < 0).sum()\n",
    "zero_sales = (train.sales == 0).sum()\n",
    "print(f\"\\nNegative sales records: {neg_sales} ({neg_sales/len(train)*100:.2f}%)\")\n",
    "print(f\"Zero sales records: {zero_sales} ({zero_sales/len(train)*100:.2f}%)\")\n",
    "\n",
    "# Sales distribution analysis (excluding zero sales)\n",
    "positive_sales = train[train.sales > 0]['sales']\n",
    "print(f\"\\n=== SALES DISTRIBUTION ANALYSIS (Sales > 0) ===\")\n",
    "print(f\"Positive sales records: {len(positive_sales):,}\")\n",
    "print(f\"Sales range: ${positive_sales.min():.2f} - ${positive_sales.max():.2f}\")\n",
    "print(f\"Mean sales: ${positive_sales.mean():.2f}\")\n",
    "print(f\"Median sales: ${positive_sales.median():.2f}\")\n",
    "print(f\"Sales std: ${positive_sales.std():.2f}\")\n",
    "\n",
    "# Sales percentiles\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(\"\\nSales percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = positive_sales.quantile(p/100)\n",
    "    print(f\"{p}th percentile: ${value:.2f}\")\n",
    "\n",
    "# Plot sales distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Raw sales distribution\n",
    "axes[0,0].hist(positive_sales, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Sales Distribution (Sales > 0)')\n",
    "axes[0,0].set_xlabel('Sales ($)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Log scale distribution\n",
    "axes[0,1].hist(positive_sales, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Sales Distribution (Log Scale, Sales > 0)')\n",
    "axes[0,1].set_xlabel('Sales ($)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_yscale('log')\n",
    "\n",
    "# Box plot\n",
    "axes[1,0].boxplot(positive_sales, vert=True)\n",
    "axes[1,0].set_title('Sales Box Plot (Sales > 0)')\n",
    "axes[1,0].set_ylabel('Sales ($)')\n",
    "\n",
    "# Sales by price ranges (excluding zero)\n",
    "price_ranges = pd.cut(positive_sales, \n",
    "                     bins=[0, 1, 5, 10, 25, 50, 100, float('inf')],\n",
    "                     labels=['$0-1', '$1-5', '$5-10', '$10-25', '$25-50', '$50-100', '$100+'])\n",
    "range_counts = price_ranges.value_counts().sort_index()\n",
    "axes[1,1].bar(range_counts.index, range_counts.values)\n",
    "axes[1,1].set_title('Sales by Price Ranges (Sales > 0)')\n",
    "axes[1,1].set_xlabel('Price Range')\n",
    "axes[1,1].set_ylabel('Number of Sales')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Price range analysis\n",
    "print(\"\\nSales by price ranges (Sales > 0):\")\n",
    "for range_label, count in range_counts.items():\n",
    "    percentage = count / len(positive_sales) * 100\n",
    "    print(f\"{range_label}: {count:,} sales ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "\n",
    "# Aggregate daily sales\n",
    "daily_sales = train.groupby('date')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Total daily sales\n",
    "axes[0,0].plot(daily_sales.date, daily_sales['sum'])\n",
    "axes[0,0].set_title('Total Daily Sales')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average daily sales\n",
    "axes[0,1].plot(daily_sales.date, daily_sales['mean'])\n",
    "axes[0,1].set_title('Average Daily Sales per Store-Product')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales distribution\n",
    "axes[1,0].hist(train.sales[train.sales > 0], bins=50, alpha=0.7)\n",
    "axes[1,0].set_title('Sales Distribution (Positive Sales Only)')\n",
    "axes[1,0].set_xlabel('Sales')\n",
    "axes[1,0].set_yscale('log')\n",
    "\n",
    "# Monthly sales trend\n",
    "monthly_sales = train.groupby(train.date.dt.to_period('M'))['sales'].sum()\n",
    "axes[1,1].plot(monthly_sales.index.astype(str), monthly_sales.values)\n",
    "axes[1,1].set_title('Monthly Sales Trend')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sales trend: {monthly_sales.iloc[-1]/monthly_sales.iloc[0]:.2f}x growth from start to end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Analiza sprzedaży według krajów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store analysis\n",
    "store_sales = train.groupby('store_nbr')['sales'].agg(['sum', 'mean', 'count'])\n",
    "store_info = store_sales.merge(stores, on='store_nbr')\n",
    "\n",
    "# Top performing stores\n",
    "print(\"Top 10 stores by total sales:\")\n",
    "print(store_info.nlargest(10, 'sum')[['sum', 'mean', 'city', 'state', 'type', 'cluster']])\n",
    "\n",
    "# Store performance by attributes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sales by store type\n",
    "type_sales = store_info.groupby('type')['sum'].mean()\n",
    "axes[0,0].bar(type_sales.index, type_sales.values)\n",
    "axes[0,0].set_title('Average Sales by Store Type')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales by state\n",
    "state_sales = store_info.groupby('state')['sum'].mean().sort_values(ascending=False).head(10)\n",
    "axes[0,1].barh(state_sales.index, state_sales.values)\n",
    "axes[0,1].set_title('Top 10 States by Average Store Sales')\n",
    "\n",
    "# Store cluster analysis\n",
    "cluster_sales = store_info.groupby('cluster')['sum'].mean()\n",
    "axes[1,0].bar(cluster_sales.index, cluster_sales.values)\n",
    "axes[1,0].set_title('Average Sales by Store Cluster')\n",
    "\n",
    "# Store count by type\n",
    "type_counts = stores['type'].value_counts()\n",
    "axes[1,1].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Store Distribution by Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStore performance varies {store_sales['sum'].max()/store_sales['sum'].min():.1f}x between best and worst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Analiza produktów i kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product family analysis\n",
    "family_sales = train.groupby('family')['sales'].agg(['sum', 'mean', 'count']).sort_values('sum', ascending=False)\n",
    "\n",
    "print(\"Top product families by total sales:\")\n",
    "print(family_sales.head(10))\n",
    "\n",
    "# Plot top families\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Top 15 families by total sales\n",
    "top_families = family_sales.head(15)\n",
    "axes[0,0].barh(range(len(top_families)), top_families['sum'])\n",
    "axes[0,0].set_yticks(range(len(top_families)))\n",
    "axes[0,0].set_yticklabels(top_families.index)\n",
    "axes[0,0].set_title('Top 15 Product Families by Total Sales')\n",
    "\n",
    "# Sales volatility (coefficient of variation)\n",
    "family_cv = train.groupby('family')['sales'].apply(lambda x: x.std() / x.mean()).sort_values(ascending=False)\n",
    "axes[0,1].bar(range(len(family_cv.head(10))), family_cv.head(10))\n",
    "axes[0,1].set_xticks(range(len(family_cv.head(10))))\n",
    "axes[0,1].set_xticklabels(family_cv.head(10).index, rotation=45, ha='right')\n",
    "axes[0,1].set_title('Most Volatile Product Families (CV)')\n",
    "\n",
    "# Family performance trend (recent vs early period)\n",
    "early_period = train[train.date < '2015-01-01'].groupby('family')['sales'].sum()\n",
    "recent_period = train[train.date >= '2016-01-01'].groupby('family')['sales'].sum()\n",
    "growth_rate = (recent_period / early_period).sort_values(ascending=False).head(10)\n",
    "\n",
    "axes[1,0].bar(range(len(growth_rate)), growth_rate)\n",
    "axes[1,0].set_xticks(range(len(growth_rate)))\n",
    "axes[1,0].set_xticklabels(growth_rate.index, rotation=45, ha='right')\n",
    "axes[1,0].set_title('Fastest Growing Product Families')\n",
    "axes[1,0].axhline(y=1, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Sales seasonality for top family\n",
    "top_family = family_sales.index[0]\n",
    "top_family_data = train[train.family == top_family]\n",
    "monthly_pattern = top_family_data.groupby(top_family_data.date.dt.month)['sales'].mean()\n",
    "axes[1,1].plot(monthly_pattern.index, monthly_pattern.values, marker='o')\n",
    "axes[1,1].set_title(f'Monthly Seasonality - {top_family}')\n",
    "axes[1,1].set_xlabel('Month')\n",
    "axes[1,1].set_xticks(range(1, 13))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Individual Product Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual product analysis - detailed look at specific products\n",
    "\n",
    "# Select interesting products for detailed analysis\n",
    "top_products = []\n",
    "for family in family_sales.head(5).index:\n",
    "    # Get representative store for each top family\n",
    "    family_data = train[train.family == family]\n",
    "    top_store = family_data.groupby('store_nbr')['sales'].sum().idxmax()\n",
    "    top_products.append((family, top_store))\n",
    "\n",
    "print(\"Selected products for detailed analysis:\")\n",
    "for family, store in top_products:\n",
    "    print(f\"- {family} at Store {store}\")\n",
    "\n",
    "# Create detailed analysis for each selected product\n",
    "fig, axes = plt.subplots(len(top_products), 3, figsize=(20, 5*len(top_products)))\n",
    "if len(top_products) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (family, store) in enumerate(top_products):\n",
    "    product_data = train[(train.family == family) & (train.store_nbr == store)].copy()\n",
    "    product_data = product_data.sort_values('date')\n",
    "    \n",
    "    # 1. Time series with trend line\n",
    "    axes[i,0].plot(product_data.date, product_data.sales, alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Add trend line\n",
    "    x_numeric = np.arange(len(product_data))\n",
    "    z = np.polyfit(x_numeric, product_data.sales, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i,0].plot(product_data.date, p(x_numeric), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    axes[i,0].set_title(f'{family} (Store {store}) - Sales Trend')\n",
    "    axes[i,0].tick_params(axis='x', rotation=45)\n",
    "    axes[i,0].set_ylabel('Sales')\n",
    "    \n",
    "    # Calculate trend statistics\n",
    "    trend_slope = z[0]\n",
    "    avg_sales = product_data.sales.mean()\n",
    "    trend_pct = (trend_slope * 365) / avg_sales * 100  # Annual trend percentage\n",
    "    \n",
    "    # 2. Seasonal patterns (monthly)\n",
    "    monthly_avg = product_data.groupby(product_data.date.dt.month)['sales'].mean()\n",
    "    axes[i,1].bar(monthly_avg.index, monthly_avg.values, alpha=0.7)\n",
    "    axes[i,1].set_title(f'{family} - Monthly Seasonality\\n(Trend: {trend_pct:+.1f}% per year)')\n",
    "    axes[i,1].set_xlabel('Month')\n",
    "    axes[i,1].set_ylabel('Average Sales')\n",
    "    axes[i,1].set_xticks(range(1, 13))\n",
    "    \n",
    "    # 3. Weekly patterns\n",
    "    product_data['weekday'] = product_data.date.dt.dayofweek\n",
    "    weekly_avg = product_data.groupby('weekday')['sales'].mean()\n",
    "    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    \n",
    "    axes[i,2].bar(range(7), weekly_avg.values, alpha=0.7)\n",
    "    axes[i,2].set_title(f'{family} - Weekly Patterns')\n",
    "    axes[i,2].set_xlabel('Day of Week')\n",
    "    axes[i,2].set_ylabel('Average Sales')\n",
    "    axes[i,2].set_xticks(range(7))\n",
    "    axes[i,2].set_xticklabels(weekday_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Product correlation analysis\n",
    "print(\"\\n=== PRODUCT CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Create a matrix of top families sales by date\n",
    "top_families_list = family_sales.head(8).index.tolist()\n",
    "family_pivot = train[train.family.isin(top_families_list)].groupby(['date', 'family'])['sales'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = family_pivot.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Product Family Sales Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most and least correlated product pairs\n",
    "corr_values = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        family1 = corr_matrix.columns[i]\n",
    "        family2 = corr_matrix.columns[j]\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        corr_values.append((family1, family2, corr_val))\n",
    "\n",
    "corr_df = pd.DataFrame(corr_values, columns=['Family1', 'Family2', 'Correlation'])\n",
    "corr_df = corr_df.sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"\\nMost correlated product families:\")\n",
    "print(corr_df.head(5)[['Family1', 'Family2', 'Correlation']])\n",
    "\n",
    "print(\"\\nLeast correlated product families:\")\n",
    "print(corr_df.tail(5)[['Family1', 'Family2', 'Correlation']])\n",
    "\n",
    "# Promotion impact analysis for individual products\n",
    "print(\"\\n=== PROMOTION IMPACT ANALYSIS ===\")\n",
    "\n",
    "# Check if we have promotion data in the dataset\n",
    "if 'onpromotion' in train.columns:\n",
    "    print(\"Analyzing promotion impact on sales...\")\n",
    "    \n",
    "    promotion_analysis = []\n",
    "    for family in top_families_list[:5]:  # Analyze top 5 families\n",
    "        family_data = train[train.family == family].copy()\n",
    "        \n",
    "        # Sales with and without promotion\n",
    "        promo_sales = family_data[family_data.onpromotion > 0]['sales']\n",
    "        no_promo_sales = family_data[family_data.onpromotion == 0]['sales']\n",
    "        \n",
    "        if len(promo_sales) > 0 and len(no_promo_sales) > 0:\n",
    "            promo_avg = promo_sales.mean()\n",
    "            no_promo_avg = no_promo_sales.mean()\n",
    "            lift = (promo_avg - no_promo_avg) / no_promo_avg * 100\n",
    "            \n",
    "            promotion_analysis.append({\n",
    "                'Family': family,\n",
    "                'Avg_Sales_No_Promo': no_promo_avg,\n",
    "                'Avg_Sales_Promo': promo_avg,\n",
    "                'Promotion_Lift_%': lift,\n",
    "                'Promo_Records': len(promo_sales)\n",
    "            })\n",
    "    \n",
    "    if promotion_analysis:\n",
    "        promo_df = pd.DataFrame(promotion_analysis)\n",
    "        promo_df = promo_df.sort_values('Promotion_Lift_%', ascending=False)\n",
    "        \n",
    "        print(\"\\nPromotion effectiveness by product family:\")\n",
    "        print(promo_df.round(2))\n",
    "        \n",
    "        # Plot promotion lift\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(promo_df)), promo_df['Promotion_Lift_%'], alpha=0.7)\n",
    "        plt.title('Promotion Lift by Product Family')\n",
    "        plt.xlabel('Product Family')\n",
    "        plt.ylabel('Sales Lift (%)')\n",
    "        plt.xticks(range(len(promo_df)), promo_df['Family'], rotation=45, ha='right')\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + (height*0.01 if height > 0 else height*0.01),\n",
    "                    f'{height:.1f}%', ha='center', va='bottom' if height > 0 else 'top')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No promotion data found for analysis.\")\n",
    "else:\n",
    "    print(\"No 'onpromotion' column found in the dataset.\")\n",
    "    \n",
    "    # Alternative: analyze sales spikes as potential promotions\n",
    "    print(\"\\nAnalyzing sales spikes as potential promotional periods...\")\n",
    "    \n",
    "    for family in top_families_list[:3]:\n",
    "        family_data = train[train.family == family].copy()\n",
    "        family_daily = family_data.groupby('date')['sales'].sum().reset_index()\n",
    "        \n",
    "        # Define spike as sales > 95th percentile\n",
    "        threshold = family_daily['sales'].quantile(0.95)\n",
    "        spike_days = family_daily[family_daily['sales'] > threshold]\n",
    "        normal_days = family_daily[family_daily['sales'] <= threshold]\n",
    "        \n",
    "        if len(spike_days) > 0:\n",
    "            spike_avg = spike_days['sales'].mean()\n",
    "            normal_avg = normal_days['sales'].mean()\n",
    "            spike_lift = (spike_avg - normal_avg) / normal_avg * 100\n",
    "            \n",
    "            print(f\"\\n{family}:\")\n",
    "            print(f\"  Normal days avg sales: ${normal_avg:,.2f}\")\n",
    "            print(f\"  Spike days avg sales: ${spike_avg:,.2f}\")\n",
    "            print(f\"  Spike lift: {spike_lift:.1f}%\")\n",
    "            print(f\"  Spike days count: {len(spike_days)} ({len(spike_days)/len(family_daily)*100:.1f}% of days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 7. External Factors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External factors analysis\n",
    "print(\"=== EXTERNAL FACTORS ANALYSIS ===\")\n",
    "\n",
    "# First, let's check what external data files are available\n",
    "import os\n",
    "data_dir = '../data/raw'\n",
    "if os.path.exists(data_dir):\n",
    "    data_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    print(f\"Available data files: {data_files}\")\n",
    "else:\n",
    "    print(\"Data directory not found. Assuming files are in current directory.\")\n",
    "    data_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "    print(f\"CSV files in current directory: {data_files}\")\n",
    "\n",
    "# Try to load external data files\n",
    "external_data = {}\n",
    "file_patterns = {\n",
    "    'oil': ['oil.csv', 'oil_prices.csv'],\n",
    "    'holidays': ['holidays_events.csv', 'holidays.csv'],\n",
    "    'stores': ['stores.csv', 'store_info.csv'],\n",
    "    'transactions': ['transactions.csv']\n",
    "}\n",
    "\n",
    "for data_type, patterns in file_patterns.items():\n",
    "    for pattern in patterns:\n",
    "        try:\n",
    "            if pattern in data_files:\n",
    "                if data_dir and os.path.exists(os.path.join(data_dir, pattern)):\n",
    "                    external_data[data_type] = pd.read_csv(os.path.join(data_dir, pattern))\n",
    "                else:\n",
    "                    external_data[data_type] = pd.read_csv(pattern)\n",
    "                print(f\"Loaded {data_type} data from {pattern}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {pattern}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded external datasets: {list(external_data.keys())}\")\n",
    "\n",
    "# Analyze each external factor\n",
    "if external_data:\n",
    "    \n",
    "    # Oil prices analysis\n",
    "    if 'oil' in external_data:\n",
    "        print(\"\\n=== OIL PRICES ANALYSIS ===\")\n",
    "        oil_data = external_data['oil'].copy()\n",
    "        print(f\"Oil data shape: {oil_data.shape}\")\n",
    "        print(f\"Oil data columns: {oil_data.columns.tolist()}\")\n",
    "        \n",
    "        if 'date' in oil_data.columns:\n",
    "            oil_data['date'] = pd.to_datetime(oil_data['date'])\n",
    "            \n",
    "            # Basic oil price statistics\n",
    "            price_col = [col for col in oil_data.columns if 'price' in col.lower() or 'oil' in col.lower()]\n",
    "            if price_col:\n",
    "                price_col = price_col[0]\n",
    "                print(f\"\\nOil price statistics ({price_col}):\")\n",
    "                print(oil_data[price_col].describe())\n",
    "                \n",
    "                # Plot oil prices over time\n",
    "                plt.figure(figsize=(15, 6))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(oil_data['date'], oil_data[price_col], linewidth=1.5)\n",
    "                plt.title('Oil Prices Over Time')\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Oil Price')\n",
    "                plt.xticks(rotation=45)\n",
    "                \n",
    "                # Oil price correlation with total sales\n",
    "                # Aggregate sales by date\n",
    "                daily_sales = train.groupby('date')['sales'].sum().reset_index()\n",
    "                daily_sales['date'] = pd.to_datetime(daily_sales['date'])\n",
    "                \n",
    "                # Merge with oil data\n",
    "                sales_oil = daily_sales.merge(oil_data, on='date', how='inner')\n",
    "                \n",
    "                if len(sales_oil) > 0:\n",
    "                    correlation = sales_oil['sales'].corr(sales_oil[price_col])\n",
    "                    print(f\"\\nCorrelation between oil prices and total sales: {correlation:.3f}\")\n",
    "                    \n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.scatter(sales_oil[price_col], sales_oil['sales'], alpha=0.6)\n",
    "                    plt.xlabel('Oil Price')\n",
    "                    plt.ylabel('Total Daily Sales')\n",
    "                    plt.title(f'Sales vs Oil Price\\n(Correlation: {correlation:.3f})')\n",
    "                    \n",
    "                    # Add trend line\n",
    "                    z = np.polyfit(sales_oil[price_col], sales_oil['sales'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    plt.plot(sales_oil[price_col], p(sales_oil[price_col]), \"r--\", alpha=0.8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    # Holidays analysis\n",
    "    if 'holidays' in external_data:\n",
    "        print(\"\\n=== HOLIDAYS ANALYSIS ===\")\n",
    "        holidays_data = external_data['holidays'].copy()\n",
    "        print(f\"Holidays data shape: {holidays_data.shape}\")\n",
    "        print(f\"Holidays data columns: {holidays_data.columns.tolist()}\")\n",
    "        \n",
    "        if 'date' in holidays_data.columns:\n",
    "            holidays_data['date'] = pd.to_datetime(holidays_data['date'])\n",
    "            \n",
    "            # Count holidays by type\n",
    "            if 'type' in holidays_data.columns:\n",
    "                holiday_types = holidays_data['type'].value_counts()\n",
    "                print(f\"\\nHoliday types:\")\n",
    "                print(holiday_types)\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                plt.subplot(2, 2, 1)\n",
    "                holiday_types.plot(kind='bar', alpha=0.7)\n",
    "                plt.title('Holiday Types Distribution')\n",
    "                plt.xticks(rotation=45)\n",
    "                \n",
    "            # Analyze sales impact of holidays\n",
    "            daily_sales = train.groupby('date')['sales'].sum().reset_index()\n",
    "            daily_sales['date'] = pd.to_datetime(daily_sales['date'])\n",
    "            \n",
    "            # Mark holiday dates\n",
    "            holiday_dates = set(holidays_data['date'].dt.date)\n",
    "            daily_sales['is_holiday'] = daily_sales['date'].dt.date.isin(holiday_dates)\n",
    "            \n",
    "            holiday_sales = daily_sales[daily_sales['is_holiday']]['sales']\n",
    "            normal_sales = daily_sales[~daily_sales['is_holiday']]['sales']\n",
    "            \n",
    "            print(f\"\\nSales comparison:\")\n",
    "            print(f\"Average sales on holidays: ${holiday_sales.mean():,.2f}\")\n",
    "            print(f\"Average sales on normal days: ${normal_sales.mean():,.2f}\")\n",
    "            \n",
    "            if len(holiday_sales) > 0 and len(normal_sales) > 0:\n",
    "                holiday_lift = (holiday_sales.mean() - normal_sales.mean()) / normal_sales.mean() * 100\n",
    "                print(f\"Holiday sales lift: {holiday_lift:+.1f}%\")\n",
    "                \n",
    "                plt.subplot(2, 2, 2)\n",
    "                plt.boxplot([normal_sales, holiday_sales], labels=['Normal Days', 'Holidays'])\n",
    "                plt.title(f'Sales Distribution\\n(Holiday Lift: {holiday_lift:+.1f}%)')\n",
    "                plt.ylabel('Daily Sales')\n",
    "                \n",
    "            # Holiday calendar heatmap\n",
    "            if len(daily_sales) > 0:\n",
    "                daily_sales['year'] = daily_sales['date'].dt.year\n",
    "                daily_sales['month'] = daily_sales['date'].dt.month\n",
    "                daily_sales['day'] = daily_sales['date'].dt.day\n",
    "                \n",
    "                # Create monthly holiday counts\n",
    "                holidays_data['year'] = holidays_data['date'].dt.year\n",
    "                holidays_data['month'] = holidays_data['date'].dt.month\n",
    "                monthly_holidays = holidays_data.groupby(['year', 'month']).size().reset_index(name='holiday_count')\n",
    "                \n",
    "                # Sample for visualization (latest year)\n",
    "                if len(monthly_holidays) > 0:\n",
    "                    latest_year = monthly_holidays['year'].max()\n",
    "                    year_holidays = monthly_holidays[monthly_holidays['year'] == latest_year]\n",
    "                    \n",
    "                    plt.subplot(2, 2, 3)\n",
    "                    if len(year_holidays) > 0:\n",
    "                        plt.bar(year_holidays['month'], year_holidays['holiday_count'], alpha=0.7)\n",
    "                        plt.title(f'Monthly Holidays Count ({latest_year})')\n",
    "                        plt.xlabel('Month')\n",
    "                        plt.ylabel('Number of Holidays')\n",
    "                        plt.xticks(range(1, 13))\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Transactions analysis\n",
    "    if 'transactions' in external_data:\n",
    "        print(\"\\n=== TRANSACTIONS ANALYSIS ===\")\n",
    "        trans_data = external_data['transactions'].copy()\n",
    "        print(f\"Transactions data shape: {trans_data.shape}\")\n",
    "        print(f\"Transactions data columns: {trans_data.columns.tolist()}\")\n",
    "        \n",
    "        if 'date' in trans_data.columns:\n",
    "            trans_data['date'] = pd.to_datetime(trans_data['date'])\n",
    "            \n",
    "            # Transactions statistics\n",
    "            trans_col = [col for col in trans_data.columns if 'transaction' in col.lower()][0]\n",
    "            print(f\"\\nTransactions statistics:\")\n",
    "            print(trans_data[trans_col].describe())\n",
    "            \n",
    "            # Merge with sales data for correlation\n",
    "            daily_sales = train.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "            daily_sales['date'] = pd.to_datetime(daily_sales['date'])\n",
    "            \n",
    "            sales_trans = daily_sales.merge(trans_data, on=['date', 'store_nbr'], how='inner')\n",
    "            \n",
    "            if len(sales_trans) > 0:\n",
    "                correlation = sales_trans['sales'].corr(sales_trans[trans_col])\n",
    "                print(f\"\\nCorrelation between transactions and sales: {correlation:.3f}\")\n",
    "                \n",
    "                plt.figure(figsize=(15, 5))\n",
    "                \n",
    "                # Time series comparison\n",
    "                plt.subplot(1, 3, 1)\n",
    "                monthly_sales = sales_trans.groupby(sales_trans['date'].dt.to_period('M'))['sales'].sum()\n",
    "                monthly_trans = sales_trans.groupby(sales_trans['date'].dt.to_period('M'))[trans_col].sum()\n",
    "                \n",
    "                ax1 = plt.gca()\n",
    "                ax2 = ax1.twinx()\n",
    "                \n",
    "                line1 = ax1.plot(monthly_sales.index.astype(str), monthly_sales.values, 'b-', alpha=0.7, label='Sales')\n",
    "                line2 = ax2.plot(monthly_trans.index.astype(str), monthly_trans.values, 'r-', alpha=0.7, label='Transactions')\n",
    "                \n",
    "                ax1.set_xlabel('Month')\n",
    "                ax1.set_ylabel('Sales', color='b')\n",
    "                ax2.set_ylabel('Transactions', color='r')\n",
    "                ax1.tick_params(axis='x', rotation=45)\n",
    "                plt.title('Sales vs Transactions Over Time')\n",
    "                \n",
    "                # Scatter plot\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.scatter(sales_trans[trans_col], sales_trans['sales'], alpha=0.5)\n",
    "                plt.xlabel('Transactions')\n",
    "                plt.ylabel('Sales')\n",
    "                plt.title(f'Sales vs Transactions\\n(Correlation: {correlation:.3f})')\n",
    "                \n",
    "                # Add trend line\n",
    "                z = np.polyfit(sales_trans[trans_col], sales_trans['sales'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                plt.plot(sales_trans[trans_col], p(sales_trans[trans_col]), \"r--\", alpha=0.8)\n",
    "                \n",
    "                # Average transaction value\n",
    "                sales_trans['avg_transaction_value'] = sales_trans['sales'] / sales_trans[trans_col]\n",
    "                sales_trans['avg_transaction_value'] = sales_trans['avg_transaction_value'].replace([np.inf, -np.inf], np.nan)\n",
    "                \n",
    "                plt.subplot(1, 3, 3)\n",
    "                avg_trans_value = sales_trans.groupby('store_nbr')['avg_transaction_value'].mean().dropna()\n",
    "                plt.hist(avg_trans_value, bins=20, alpha=0.7, edgecolor='black')\n",
    "                plt.xlabel('Average Transaction Value')\n",
    "                plt.ylabel('Number of Stores')\n",
    "                plt.title('Distribution of Avg Transaction Value by Store')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nAverage transaction value statistics:\")\n",
    "                print(avg_trans_value.describe())\n",
    "    \n",
    "    # Store information analysis\n",
    "    if 'stores' in external_data:\n",
    "        print(\"\\n=== STORE INFORMATION ANALYSIS ===\")\n",
    "        stores_data = external_data['stores'].copy()\n",
    "        print(f\"Stores data shape: {stores_data.shape}\")\n",
    "        print(f\"Stores data columns: {stores_data.columns.tolist()}\")\n",
    "        \n",
    "        # Merge store info with sales\n",
    "        store_sales = train.groupby('store_nbr')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "        store_analysis = store_sales.merge(stores_data, left_on='store_nbr', right_on='store_nbr', how='left')\n",
    "        \n",
    "        # Analyze by store attributes\n",
    "        for col in stores_data.columns:\n",
    "            if col != 'store_nbr' and stores_data[col].dtype in ['object', 'category']:\n",
    "                print(f\"\\nSales by {col}:\")\n",
    "                group_stats = store_analysis.groupby(col)['sum'].agg(['mean', 'count'])\n",
    "                print(group_stats)\n",
    "        \n",
    "        # Visualize store performance by attributes\n",
    "        categorical_cols = [col for col in stores_data.columns \n",
    "                          if col != 'store_nbr' and stores_data[col].dtype in ['object', 'category']]\n",
    "        \n",
    "        if categorical_cols:\n",
    "            n_cols = min(3, len(categorical_cols))\n",
    "            fig, axes = plt.subplots(1, n_cols, figsize=(6*n_cols, 6))\n",
    "            if n_cols == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, col in enumerate(categorical_cols[:n_cols]):\n",
    "                group_sales = store_analysis.groupby(col)['sum'].mean()\n",
    "                group_sales.plot(kind='bar', ax=axes[i], alpha=0.7)\n",
    "                axes[i].set_title(f'Average Total Sales by {col}')\n",
    "                axes[i].set_ylabel('Average Total Sales')\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No external data files found. Performing analysis on available train data only.\")\n",
    "    \n",
    "    # Alternative analysis using patterns in the training data\n",
    "    print(\"\\n=== TEMPORAL PATTERNS ANALYSIS ===\")\n",
    "    \n",
    "    # Day of week patterns\n",
    "    train['weekday'] = train['date'].dt.dayofweek\n",
    "    weekday_sales = train.groupby('weekday')['sales'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    plt.bar(range(7), weekday_sales.values, alpha=0.7)\n",
    "    plt.title('Average Sales by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Average Sales')\n",
    "    plt.xticks(range(7), weekday_names)\n",
    "    \n",
    "    # Monthly patterns\n",
    "    monthly_sales = train.groupby(train['date'].dt.month)['sales'].mean()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(monthly_sales.index, monthly_sales.values, alpha=0.7)\n",
    "    plt.title('Average Sales by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Sales')\n",
    "    plt.xticks(range(1, 13))\n",
    "    \n",
    "    # Year-over-year growth\n",
    "    yearly_sales = train.groupby(train['date'].dt.year)['sales'].sum()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    yearly_growth = yearly_sales.pct_change() * 100\n",
    "    plt.bar(yearly_growth.index[1:], yearly_growth.values[1:], alpha=0.7)\n",
    "    plt.title('Year-over-Year Sales Growth')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Growth (%)')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nWeekday sales patterns:\")\n",
    "    for i, day in enumerate(weekday_names):\n",
    "        print(f\"{day}: ${weekday_sales.iloc[i]:,.2f}\")\n",
    "    \n",
    "    print(\"\\nYearly sales totals:\")\n",
    "    for year, total in yearly_sales.items():\n",
    "        print(f\"{year}: ${total:,.2f}\")\n",
    "\n",
    "print(\"\\n=== DATA EXPLORATION SUMMARY ===\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"1. Dataset contains sales data for 54 stores and 33 product families\")\n",
    "print(f\"2. Time period: {train['date'].min()} to {train['date'].max()}\")\n",
    "print(f\"3. Total sales records: {len(train):,}\")\n",
    "print(f\"4. Zero sales records: {(train['sales'] == 0).sum():,} ({(train['sales'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"5. Average daily sales per store-product: ${train['sales'].mean():.2f}\")\n",
    "print(f\"6. Total sales value: ${train['sales'].sum():,.2f}\")\n",
    "print(\"\\nNext steps: Data preprocessing, feature engineering, and model development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This comprehensive data exploration has revealed several key insights about the Store Sales Time Series dataset:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Data Structure**: Complete grid format with 54 stores × 33 product families × 1,684 days\n",
    "2. **Data Quality**: ~31% zero sales records, indicating either no stock or no demand\n",
    "3. **Sales Distribution**: Wide range from $0.01 to $3,502, with most sales under $20\n",
    "4. **Top Performers**: GROCERY I, BEVERAGES, and PRODUCE are the highest-selling families\n",
    "5. **Store Patterns**: Significant variation in performance across stores and locations\n",
    "6. **Seasonality**: Clear monthly and weekly patterns in sales data\n",
    "7. **Growth Trends**: Generally positive trends for most product families\n",
    "8. **External Factors**: Oil prices, holidays, and transaction counts show varying correlations with sales\n",
    "\n",
    "### Implications for Modeling:\n",
    "- Need to handle zero sales appropriately (separate model or special treatment)\n",
    "- Store and product family clustering could improve predictions\n",
    "- Seasonal patterns should be incorporated as features\n",
    "- External factors like holidays and promotions are important\n",
    "- Individual product analysis reveals unique patterns that may require family-specific models\n",
    "\n",
    "### Next Steps:\n",
    "1. **Data Preprocessing**: Handle zeros, outliers, and missing values\n",
    "2. **Feature Engineering**: Create lag features, rolling statistics, seasonal features\n",
    "3. **Baseline Models**: Start with simple approaches (moving averages, linear regression)\n",
    "4. **Advanced Models**: Implement neural networks (LSTM, CNN, Transformer-based)\n",
    "5. **Model Evaluation**: Use appropriate time series validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual product family deep dive\n",
    "print(\"=== INDIVIDUAL PRODUCT FAMILY ANALYSIS ===\")\n",
    "\n",
    "# Select top 6 families for detailed analysis\n",
    "top_6_families = family_sales.head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, family in enumerate(top_6_families):\n",
    "    family_data = train[train.family == family]\n",
    "    \n",
    "    # Daily sales for this family\n",
    "    daily_family_sales = family_data.groupby('date')['sales'].sum()\n",
    "    \n",
    "    axes[i].plot(daily_family_sales.index, daily_family_sales.values, alpha=0.7)\n",
    "    axes[i].set_title(f'{family}\\nTotal Sales: ${family_sales.loc[family, \"sum\"]:,.0f}')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(range(len(daily_family_sales)), daily_family_sales.values, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i].plot(daily_family_sales.index, p(range(len(daily_family_sales))), \n",
    "                \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonality comparison for top families\n",
    "print(\"\\n=== SEASONAL PATTERNS COMPARISON ===\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, family in enumerate(top_6_families):\n",
    "    family_data = train[train.family == family]\n",
    "    \n",
    "    # Monthly seasonality\n",
    "    monthly_pattern = family_data.groupby(family_data.date.dt.month)['sales'].mean()\n",
    "    \n",
    "    axes[i].plot(monthly_pattern.index, monthly_pattern.values, marker='o', linewidth=2, markersize=8)\n",
    "    axes[i].set_title(f'{family} - Monthly Pattern')\n",
    "    axes[i].set_xlabel('Month')\n",
    "    axes[i].set_ylabel('Average Sales')\n",
    "    axes[i].set_xticks(range(1, 13))\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight peak months\n",
    "    peak_month = monthly_pattern.idxmax()\n",
    "    axes[i].scatter(peak_month, monthly_pattern[peak_month], \n",
    "                   color='red', s=100, zorder=5)\n",
    "    axes[i].annotate(f'Peak: {peak_month}', \n",
    "                    xy=(peak_month, monthly_pattern[peak_month]),\n",
    "                    xytext=(peak_month+1, monthly_pattern[peak_month]*1.1),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weekly patterns for top families\n",
    "print(\"\\n=== WEEKLY PATTERNS COMPARISON ===\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "for i, family in enumerate(top_6_families):\n",
    "    family_data = train[train.family == family]\n",
    "    \n",
    "    # Weekly pattern (0=Monday, 6=Sunday)\n",
    "    weekly_pattern = family_data.groupby(family_data.date.dt.dayofweek)['sales'].mean()\n",
    "    \n",
    "    bars = axes[i].bar(range(7), weekly_pattern.values, alpha=0.7)\n",
    "    axes[i].set_title(f'{family} - Weekly Pattern')\n",
    "    axes[i].set_xlabel('Day of Week')\n",
    "    axes[i].set_ylabel('Average Sales')\n",
    "    axes[i].set_xticks(range(7))\n",
    "    axes[i].set_xticklabels([day[:3] for day in days_of_week], rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Highlight weekend days\n",
    "    bars[5].set_color('orange')  # Saturday\n",
    "    bars[6].set_color('red')     # Sunday\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Product family statistics summary\n",
    "print(\"\\n=== TOP FAMILIES SUMMARY STATISTICS ===\")\n",
    "for family in top_6_families:\n",
    "    family_data = train[train.family == family]\n",
    "    total_sales = family_data['sales'].sum()\n",
    "    avg_daily = family_data.groupby('date')['sales'].sum().mean()\n",
    "    zero_days = (family_data.groupby('date')['sales'].sum() == 0).sum()\n",
    "    peak_day = family_data.groupby('date')['sales'].sum().max()\n",
    "    \n",
    "    print(f\"\\n{family}:\")\n",
    "    print(f\"  Total Sales: ${total_sales:,.0f}\")\n",
    "    print(f\"  Avg Daily Sales: ${avg_daily:.0f}\")\n",
    "    print(f\"  Days with Zero Sales: {zero_days}\")\n",
    "    print(f\"  Peak Daily Sales: ${peak_day:.0f}\")\n",
    "    print(f\"  Sales Volatility (CV): {family_cv[family]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product family correlation analysis\n",
    "print(\"\\n=== PRODUCT FAMILY CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Create daily sales matrix for families\n",
    "family_daily_sales = train.groupby(['date', 'family'])['sales'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate correlation matrix for top 10 families\n",
    "top_10_families = family_sales.head(10).index\n",
    "corr_matrix = family_daily_sales[top_10_families].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "ax.set_title('Product Family Sales Correlation Matrix (Top 10)', fontsize=14, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most and least correlated pairs\n",
    "corr_pairs = []\n",
    "for i in range(len(top_10_families)):\n",
    "    for j in range(i+1, len(top_10_families)):\n",
    "        family1, family2 = top_10_families[i], top_10_families[j]\n",
    "        corr_val = corr_matrix.loc[family1, family2]\n",
    "        corr_pairs.append((family1, family2, corr_val))\n",
    "\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"\\nMost correlated product families:\")\n",
    "for family1, family2, corr in corr_pairs[:5]:\n",
    "    print(f\"{family1} & {family2}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nLeast correlated product families:\")\n",
    "for family1, family2, corr in corr_pairs[-5:]:\n",
    "    print(f\"{family1} & {family2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promotion impact analysis by product family\n",
    "print(\"\\n=== PROMOTION IMPACT ANALYSIS ===\")\n",
    "\n",
    "# Calculate promotion statistics for top families\n",
    "promotion_stats = []\n",
    "for family in top_6_families:\n",
    "    family_data = train[train.family == family]\n",
    "    \n",
    "    # Sales with and without promotion\n",
    "    promo_sales = family_data[family_data.onpromotion == 1]['sales']\n",
    "    no_promo_sales = family_data[family_data.onpromotion == 0]['sales']\n",
    "    \n",
    "    # Remove zeros for better comparison\n",
    "    promo_sales_pos = promo_sales[promo_sales > 0]\n",
    "    no_promo_sales_pos = no_promo_sales[no_promo_sales > 0]\n",
    "    \n",
    "    if len(promo_sales_pos) > 0 and len(no_promo_sales_pos) > 0:\n",
    "        promo_mean = promo_sales_pos.mean()\n",
    "        no_promo_mean = no_promo_sales_pos.mean()\n",
    "        lift = (promo_mean / no_promo_mean - 1) * 100\n",
    "        \n",
    "        promotion_stats.append({\n",
    "            'family': family,\n",
    "            'promo_mean': promo_mean,\n",
    "            'no_promo_mean': no_promo_mean,\n",
    "            'lift_percent': lift,\n",
    "            'promo_records': len(promo_sales),\n",
    "            'total_records': len(family_data)\n",
    "        })\n",
    "\n",
    "# Create promotion impact visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Promotion lift chart\n",
    "families = [stat['family'] for stat in promotion_stats]\n",
    "lifts = [stat['lift_percent'] for stat in promotion_stats]\n",
    "\n",
    "bars = axes[0].barh(families, lifts, color=['green' if x > 0 else 'red' for x in lifts])\n",
    "axes[0].set_title('Promotion Lift by Product Family (%)')\n",
    "axes[0].set_xlabel('Sales Lift (%)')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, lift) in enumerate(zip(bars, lifts)):\n",
    "    axes[0].text(lift + (5 if lift > 0 else -5), bar.get_y() + bar.get_height()/2, \n",
    "                f'{lift:.1f}%', va='center', ha='left' if lift > 0 else 'right')\n",
    "\n",
    "# Promotion frequency chart\n",
    "promo_freq = [stat['promo_records'] / stat['total_records'] * 100 for stat in promotion_stats]\n",
    "\n",
    "bars2 = axes[1].barh(families, promo_freq, alpha=0.7, color='blue')\n",
    "axes[1].set_title('Promotion Frequency by Product Family (%)')\n",
    "axes[1].set_xlabel('% of Records on Promotion')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, freq in zip(bars2, promo_freq):\n",
    "    axes[1].text(freq + 1, bar.get_y() + bar.get_height()/2, \n",
    "                f'{freq:.1f}%', va='center', ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print promotion statistics\n",
    "print(\"\\nPromotion impact summary:\")\n",
    "for stat in promotion_stats:\n",
    "    print(f\"\\n{stat['family']}:\")\n",
    "    print(f\"  Average sales (no promo): ${stat['no_promo_mean']:.2f}\")\n",
    "    print(f\"  Average sales (with promo): ${stat['promo_mean']:.2f}\")\n",
    "    print(f\"  Promotion lift: {stat['lift_percent']:.1f}%\")\n",
    "    print(f\"  Promotion frequency: {stat['promo_records']/stat['total_records']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Individual Product Analysis Summary:\n",
    "\n",
    "**Key Insights from Product-Level Analysis:**\n",
    "\n",
    "1. **Sales Trends**: Each product family shows distinct growth patterns and seasonal behaviors\n",
    "2. **Seasonality**: Clear monthly and weekly patterns vary significantly between product categories\n",
    "3. **Correlations**: Some product families are highly correlated (substitute/complementary products)\n",
    "4. **Promotion Impact**: Promotional effectiveness varies dramatically across product families\n",
    "5. **Volatility**: Different product categories show varying levels of sales predictability\n",
    "\n",
    "**Business Implications:**\n",
    "- Product-specific forecasting models may be necessary\n",
    "- Promotional strategies should be tailored by product family\n",
    "- Cross-selling opportunities exist for highly correlated products\n",
    "- Seasonal inventory planning needs to account for family-specific patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 6. External Factors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oil price analysis\n",
    "print(f\"Oil data coverage: {oil.date.min()} to {oil.date.max()}\")\n",
    "print(f\"Missing oil price days: {oil.dcoilwtico.isnull().sum()}\")\n",
    "\n",
    "# Merge with daily sales for correlation\n",
    "daily_oil = daily_sales.merge(oil, on='date', how='left')\n",
    "correlation = daily_oil[['sum', 'dcoilwtico']].corr().iloc[0,1]\n",
    "print(f\"Correlation between oil prices and total sales: {correlation:.3f}\")\n",
    "\n",
    "# Holiday analysis\n",
    "print(f\"\\nHoliday types: {holidays.type.value_counts()}\")\n",
    "print(f\"Holiday impact on national level: {holidays.locale.value_counts()}\")\n",
    "\n",
    "# Plot external factors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Oil prices over time\n",
    "axes[0,0].plot(oil.date, oil.dcoilwtico)\n",
    "axes[0,0].set_title('Oil Prices Over Time')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales vs oil prices scatter\n",
    "valid_data = daily_oil.dropna()\n",
    "axes[0,1].scatter(valid_data.dcoilwtico, valid_data['sum'], alpha=0.6)\n",
    "axes[0,1].set_xlabel('Oil Price')\n",
    "axes[0,1].set_ylabel('Total Daily Sales')\n",
    "axes[0,1].set_title(f'Sales vs Oil Price (r={correlation:.3f})')\n",
    "\n",
    "# Holiday frequency by month\n",
    "holiday_monthly = holidays.groupby(holidays.date.dt.month).size()\n",
    "axes[1,0].bar(holiday_monthly.index, holiday_monthly.values)\n",
    "axes[1,0].set_title('Holidays by Month')\n",
    "axes[1,0].set_xlabel('Month')\n",
    "axes[1,0].set_xticks(range(1, 13))\n",
    "\n",
    "# Transaction pattern\n",
    "if not transactions.empty:\n",
    "    daily_transactions = transactions.groupby('date')['transactions'].sum()\n",
    "    axes[1,1].plot(daily_transactions.index, daily_transactions.values)\n",
    "    axes[1,1].set_title('Daily Transaction Volume')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No transaction data', ha='center', va='center')\n",
    "    axes[1,1].set_title('Transaction Data Not Available')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 6. Wnioski z analizy eksploracyjnej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "**Data Quality:**\n",
    "- Dataset spans multiple years with consistent daily records\n",
    "- Oil price data has some missing values\n",
    "- First day of a year always has 0 sales \n",
    "\n",
    "**Sales Patterns:**\n",
    "- Clear growth trend over time\n",
    "- Seasonal patterns visible in monthly aggregations\n",
    "- High variability between stores and product families\n",
    "\n",
    "**Store Performance:**\n",
    "- Significant performance differences across store types and locations\n",
    "- Store clusters show distinct sales patterns\n",
    "- Geographic concentration affects performance\n",
    "\n",
    "**Product Insights:**\n",
    "- Top product families dominate total sales\n",
    "- Different families show varying seasonality and growth rates\n",
    "- Some categories are more volatile than others\n",
    "\n",
    "**External Factors:**\n",
    "- Oil prices show some correlation with sales (higher the price -> lower the sales)\n",
    "- Holidays concentrated in certain months\n",
    "- Transaction volume tracks with sales patterns\n",
    "\n",
    "### Modeling Implications:\n",
    "- Consider store-specific models or clustering\n",
    "- Include external factors (oil prices, holidays)\n",
    "- Account for seasonality and trends\n",
    "- Handle negative sales appropriately\n",
    "- Feature engineering for location and store characteristics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

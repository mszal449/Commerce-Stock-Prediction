{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Colab url\n",
    "https://colab.research.google.com/drive/1R6nEezCr-MayFCobKtLLtqjCWQ3npObu?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Store Sales Data Exploration\n",
    "\n",
    "Exploratory Data Analysis for the Store Sales Time Series Forecasting dataset from Kaggle.\n",
    "\n",
    "## Dataset Overview:\n",
    "- **train.csv**: Sales data with store, item family, and dates\n",
    "- **stores.csv**: Store metadata (city, state, type, cluster)\n",
    "- **oil.csv**: Daily oil prices (economic indicator)\n",
    "- **holidays_events.csv**: Holiday and event information\n",
    "- **transactions.csv**: Number of transactions per store/date\n",
    "\n",
    "## Analysis Goals:\n",
    "- Understand data structure and quality\n",
    "- Identify sales patterns and trends\n",
    "- Explore seasonality and holidays impact\n",
    "- Analyze store and product performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = Path('../data/raw')\n",
    "\n",
    "print(f\"Data files available: {list(DATA_PATH.glob('*.csv'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(DATA_PATH / 'train.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv(DATA_PATH / 'stores.csv')\n",
    "oil = pd.read_csv(DATA_PATH / 'oil.csv', parse_dates=['date'])\n",
    "holidays = pd.read_csv(DATA_PATH / 'holidays_events.csv', parse_dates=['date'])\n",
    "transactions = pd.read_csv(DATA_PATH / 'transactions.csv', parse_dates=['date'])\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "for name, df in [('train', train), ('stores', stores), ('oil', oil), ('holidays', holidays), ('transactions', transactions)]:\n",
    "    print(f\"{name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data sample:\")\n",
    "print(train.head(50))\n",
    "print(\"\\nData types:\")\n",
    "print(train.dtypes)\n",
    "print(f\"\\nDate range: {train.date.min()} to {train.date.max()}\")\n",
    "print(f\"Unique stores: {train.store_nbr.nunique()}\")\n",
    "print(f\"Product families: {train.family.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"Missing values:\")\n",
    "for name, df in [('train', train), ('stores', stores), ('oil', oil), ('holidays', holidays), ('transactions', transactions)]:\n",
    "    missing = df.isnull().sum().sum()\n",
    "    print(f\"{name}: {missing} ({missing/df.size*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTrain dataset statistics:\")\n",
    "print(train.describe())\n",
    "\n",
    "neg_sales = (train.sales < 0).sum()\n",
    "zero_sales = (train.sales == 0).sum()\n",
    "print(f\"\\nNegative sales records: {neg_sales} ({neg_sales/len(train)*100:.2f}%)\")\n",
    "print(f\"Zero sales records: {zero_sales} ({zero_sales/len(train)*100:.2f}%)\")\n",
    "\n",
    "# Sales distribution analysis (excluding zero sales)\n",
    "positive_sales = train[train.sales > 0]['sales']\n",
    "print(f\"\\n=== SALES DISTRIBUTION ANALYSIS (Sales > 0) ===\")\n",
    "print(f\"Positive sales records: {len(positive_sales):,}\")\n",
    "print(f\"Sales range: ${positive_sales.min():.2f} - ${positive_sales.max():.2f}\")\n",
    "print(f\"Mean sales: ${positive_sales.mean():.2f}\")\n",
    "print(f\"Median sales: ${positive_sales.median():.2f}\")\n",
    "print(f\"Sales std: ${positive_sales.std():.2f}\")\n",
    "\n",
    "# Sales percentiles\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(\"\\nSales percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = positive_sales.quantile(p/100)\n",
    "    print(f\"{p}th percentile: ${value:.2f}\")\n",
    "\n",
    "# Plot sales distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Raw sales distribution\n",
    "axes[0,0].hist(positive_sales, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Sales Distribution (Sales > 0)')\n",
    "axes[0,0].set_xlabel('Sales ($)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Log scale distribution\n",
    "axes[0,1].hist(positive_sales, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Sales Distribution (Log Scale, Sales > 0)')\n",
    "axes[0,1].set_xlabel('Sales ($)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_yscale('log')\n",
    "\n",
    "# Box plot\n",
    "axes[1,0].boxplot(positive_sales, vert=True)\n",
    "axes[1,0].set_title('Sales Box Plot (Sales > 0)')\n",
    "axes[1,0].set_ylabel('Sales ($)')\n",
    "\n",
    "# Sales by price ranges (excluding zero)\n",
    "price_ranges = pd.cut(positive_sales, \n",
    "                     bins=[0, 1, 5, 10, 25, 50, 100, float('inf')],\n",
    "                     labels=['$0-1', '$1-5', '$5-10', '$10-25', '$25-50', '$50-100', '$100+'])\n",
    "range_counts = price_ranges.value_counts().sort_index()\n",
    "axes[1,1].bar(range_counts.index, range_counts.values)\n",
    "axes[1,1].set_title('Sales by Price Ranges (Sales > 0)')\n",
    "axes[1,1].set_xlabel('Price Range')\n",
    "axes[1,1].set_ylabel('Number of Sales')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Price range analysis\n",
    "print(\"\\nSales by price ranges (Sales > 0):\")\n",
    "for range_label, count in range_counts.items():\n",
    "    percentage = count / len(positive_sales) * 100\n",
    "    print(f\"{range_label}: {count:,} sales ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "\n",
    "# Aggregate daily sales\n",
    "daily_sales = train.groupby('date')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Total daily sales\n",
    "axes[0,0].plot(daily_sales.date, daily_sales['sum'])\n",
    "axes[0,0].set_title('Total Daily Sales')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average daily sales\n",
    "axes[0,1].plot(daily_sales.date, daily_sales['mean'])\n",
    "axes[0,1].set_title('Average Daily Sales per Store-Product')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales distribution\n",
    "axes[1,0].hist(train.sales[train.sales > 0], bins=50, alpha=0.7)\n",
    "axes[1,0].set_title('Sales Distribution (Positive Sales Only)')\n",
    "axes[1,0].set_xlabel('Sales')\n",
    "axes[1,0].set_yscale('log')\n",
    "\n",
    "# Monthly sales trend\n",
    "monthly_sales = train.groupby(train.date.dt.to_period('M'))['sales'].sum()\n",
    "axes[1,1].plot(monthly_sales.index.astype(str), monthly_sales.values)\n",
    "axes[1,1].set_title('Monthly Sales Trend')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sales trend: {monthly_sales.iloc[-1]/monthly_sales.iloc[0]:.2f}x growth from start to end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Analiza sprzedaży według krajów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store analysis\n",
    "store_sales = train.groupby('store_nbr')['sales'].agg(['sum', 'mean', 'count'])\n",
    "store_info = store_sales.merge(stores, on='store_nbr')\n",
    "\n",
    "# Top performing stores\n",
    "print(\"Top 10 stores by total sales:\")\n",
    "print(store_info.nlargest(10, 'sum')[['sum', 'mean', 'city', 'state', 'type', 'cluster']])\n",
    "\n",
    "# Store performance by attributes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sales by store type\n",
    "type_sales = store_info.groupby('type')['sum'].mean()\n",
    "axes[0,0].bar(type_sales.index, type_sales.values)\n",
    "axes[0,0].set_title('Average Sales by Store Type')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales by state\n",
    "state_sales = store_info.groupby('state')['sum'].mean().sort_values(ascending=False).head(10)\n",
    "axes[0,1].barh(state_sales.index, state_sales.values)\n",
    "axes[0,1].set_title('Top 10 States by Average Store Sales')\n",
    "\n",
    "# Store cluster analysis\n",
    "cluster_sales = store_info.groupby('cluster')['sum'].mean()\n",
    "axes[1,0].bar(cluster_sales.index, cluster_sales.values)\n",
    "axes[1,0].set_title('Average Sales by Store Cluster')\n",
    "\n",
    "# Store count by type\n",
    "type_counts = stores['type'].value_counts()\n",
    "axes[1,1].pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Store Distribution by Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStore performance varies {store_sales['sum'].max()/store_sales['sum'].min():.1f}x between best and worst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Product and category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product family analysis\n",
    "family_sales = train.groupby('family')['sales'].agg(['sum', 'mean', 'count']).sort_values('sum', ascending=False)\n",
    "\n",
    "print(\"Top product families by total sales:\")\n",
    "print(family_sales.head(10))\n",
    "\n",
    "# Plot top families\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Top 15 families by total sales\n",
    "top_families = family_sales.head(15)\n",
    "axes[0,0].barh(range(len(top_families)), top_families['sum'])\n",
    "axes[0,0].set_yticks(range(len(top_families)))\n",
    "axes[0,0].set_yticklabels(top_families.index)\n",
    "axes[0,0].set_title('Top 15 Product Families by Total Sales')\n",
    "\n",
    "# Sales volatility (coefficient of variation)\n",
    "family_cv = train.groupby('family')['sales'].apply(lambda x: x.std() / x.mean()).sort_values(ascending=False)\n",
    "axes[0,1].bar(range(len(family_cv.head(10))), family_cv.head(10))\n",
    "axes[0,1].set_xticks(range(len(family_cv.head(10))))\n",
    "axes[0,1].set_xticklabels(family_cv.head(10).index, rotation=45, ha='right')\n",
    "axes[0,1].set_title('Most Volatile Product Families (CV)')\n",
    "\n",
    "# Family performance trend (recent vs early period)\n",
    "early_period = train[train.date < '2015-01-01'].groupby('family')['sales'].sum()\n",
    "recent_period = train[train.date >= '2016-01-01'].groupby('family')['sales'].sum()\n",
    "growth_rate = (recent_period / early_period).sort_values(ascending=False).head(10)\n",
    "\n",
    "axes[1,0].bar(range(len(growth_rate)), growth_rate)\n",
    "axes[1,0].set_xticks(range(len(growth_rate)))\n",
    "axes[1,0].set_xticklabels(growth_rate.index, rotation=45, ha='right')\n",
    "axes[1,0].set_title('Fastest Growing Product Families')\n",
    "axes[1,0].axhline(y=1, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Sales seasonality for top family\n",
    "top_family = family_sales.index[0]\n",
    "top_family_data = train[train.family == top_family]\n",
    "monthly_pattern = top_family_data.groupby(top_family_data.date.dt.month)['sales'].mean()\n",
    "axes[1,1].plot(monthly_pattern.index, monthly_pattern.values, marker='o')\n",
    "axes[1,1].set_title(f'Monthly Seasonality - {top_family}')\n",
    "axes[1,1].set_xlabel('Month')\n",
    "axes[1,1].set_xticks(range(1, 13))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Individual Product Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual product analysis\n",
    "\n",
    "# Select interesting products for detailed analysis\n",
    "top_products = []\n",
    "for family in family_sales.head(5).index:\n",
    "    # Get representative store for each top family\n",
    "    family_data = train[train.family == family]\n",
    "    top_store = family_data.groupby('store_nbr')['sales'].sum().idxmax()\n",
    "    top_products.append((family, top_store))\n",
    "\n",
    "print(\"Selected products for detailed analysis:\")\n",
    "for family, store in top_products:\n",
    "    print(f\"- {family} at Store {store}\")\n",
    "\n",
    "# Create detailed analysis for each selected product\n",
    "fig, axes = plt.subplots(len(top_products), 3, figsize=(20, 5*len(top_products)))\n",
    "if len(top_products) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (family, store) in enumerate(top_products):\n",
    "    product_data = train[(train.family == family) & (train.store_nbr == store)].copy()\n",
    "    product_data = product_data.sort_values('date')\n",
    "    \n",
    "    # 1. Time series with trend line\n",
    "    axes[i,0].plot(product_data.date, product_data.sales, alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Add trend line\n",
    "    x_numeric = np.arange(len(product_data))\n",
    "    z = np.polyfit(x_numeric, product_data.sales, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i,0].plot(product_data.date, p(x_numeric), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    axes[i,0].set_title(f'{family} (Store {store}) - Sales Trend')\n",
    "    axes[i,0].tick_params(axis='x', rotation=45)\n",
    "    axes[i,0].set_ylabel('Sales')\n",
    "    \n",
    "    # Calculate trend statistics\n",
    "    trend_slope = z[0]\n",
    "    avg_sales = product_data.sales.mean()\n",
    "    trend_pct = (trend_slope * 365) / avg_sales * 100  # Annual trend percentage\n",
    "    \n",
    "    # 2. Seasonal patterns (monthly)\n",
    "    monthly_avg = product_data.groupby(product_data.date.dt.month)['sales'].mean()\n",
    "    axes[i,1].bar(monthly_avg.index, monthly_avg.values, alpha=0.7)\n",
    "    axes[i,1].set_title(f'{family} - Monthly Seasonality\\n(Trend: {trend_pct:+.1f}% per year)')\n",
    "    axes[i,1].set_xlabel('Month')\n",
    "    axes[i,1].set_ylabel('Average Sales')\n",
    "    axes[i,1].set_xticks(range(1, 13))\n",
    "    \n",
    "    # 3. Weekly patterns\n",
    "    product_data['weekday'] = product_data.date.dt.dayofweek\n",
    "    weekly_avg = product_data.groupby('weekday')['sales'].mean()\n",
    "    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    \n",
    "    axes[i,2].bar(range(7), weekly_avg.values, alpha=0.7)\n",
    "    axes[i,2].set_title(f'{family} - Weekly Patterns')\n",
    "    axes[i,2].set_xlabel('Day of Week')\n",
    "    axes[i,2].set_ylabel('Average Sales')\n",
    "    axes[i,2].set_xticks(range(7))\n",
    "    axes[i,2].set_xticklabels(weekday_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Product correlation analysis\n",
    "print(\"\\n=== PRODUCT CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Create a matrix of top families sales by date\n",
    "top_families_list = family_sales.head(8).index.tolist()\n",
    "family_pivot = train[train.family.isin(top_families_list)].groupby(['date', 'family'])['sales'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = family_pivot.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Product Family Sales Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most and least correlated product pairs\n",
    "corr_values = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        family1 = corr_matrix.columns[i]\n",
    "        family2 = corr_matrix.columns[j]\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        corr_values.append((family1, family2, corr_val))\n",
    "\n",
    "corr_df = pd.DataFrame(corr_values, columns=['Family1', 'Family2', 'Correlation'])\n",
    "corr_df = corr_df.sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"\\nMost correlated product families:\")\n",
    "print(corr_df.head(5)[['Family1', 'Family2', 'Correlation']])\n",
    "\n",
    "print(\"\\nLeast correlated product families:\")\n",
    "print(corr_df.tail(5)[['Family1', 'Family2', 'Correlation']])\n",
    "\n",
    "# Promotion impact analysis for individual products\n",
    "print(\"\\n=== PROMOTION IMPACT ANALYSIS ===\")\n",
    "\n",
    "# Check if we have promotion data in the dataset\n",
    "if 'onpromotion' in train.columns:\n",
    "    print(\"Analyzing promotion impact on sales...\")\n",
    "    \n",
    "    promotion_analysis = []\n",
    "    for family in top_families_list[:5]:  # Analyze top 5 families\n",
    "        family_data = train[train.family == family].copy()\n",
    "        \n",
    "        # Sales with and without promotion\n",
    "        promo_sales = family_data[family_data.onpromotion > 0]['sales']\n",
    "        no_promo_sales = family_data[family_data.onpromotion == 0]['sales']\n",
    "        \n",
    "        if len(promo_sales) > 0 and len(no_promo_sales) > 0:\n",
    "            promo_avg = promo_sales.mean()\n",
    "            no_promo_avg = no_promo_sales.mean()\n",
    "            lift = (promo_avg - no_promo_avg) / no_promo_avg * 100\n",
    "            \n",
    "            promotion_analysis.append({\n",
    "                'Family': family,\n",
    "                'Avg_Sales_No_Promo': no_promo_avg,\n",
    "                'Avg_Sales_Promo': promo_avg,\n",
    "                'Promotion_Lift_%': lift,\n",
    "                'Promo_Records': len(promo_sales)\n",
    "            })\n",
    "    \n",
    "    if promotion_analysis:\n",
    "        promo_df = pd.DataFrame(promotion_analysis)\n",
    "        promo_df = promo_df.sort_values('Promotion_Lift_%', ascending=False)\n",
    "        \n",
    "        print(\"\\nPromotion effectiveness by product family:\")\n",
    "        print(promo_df.round(2))\n",
    "        \n",
    "        # Plot promotion lift\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(promo_df)), promo_df['Promotion_Lift_%'], alpha=0.7)\n",
    "        plt.title('Promotion Lift by Product Family')\n",
    "        plt.xlabel('Product Family')\n",
    "        plt.ylabel('Sales Lift (%)')\n",
    "        plt.xticks(range(len(promo_df)), promo_df['Family'], rotation=45, ha='right')\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + (height*0.01 if height > 0 else height*0.01),\n",
    "                    f'{height:.1f}%', ha='center', va='bottom' if height > 0 else 'top')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No promotion data found for analysis.\")\n",
    "else:\n",
    "    print(\"No 'onpromotion' column found in the dataset.\")\n",
    "    \n",
    "    # Alternative: analyze sales spikes as potential promotions\n",
    "    print(\"\\nAnalyzing sales spikes as potential promotional periods...\")\n",
    "    \n",
    "    for family in top_families_list[:3]:\n",
    "        family_data = train[train.family == family].copy()\n",
    "        family_daily = family_data.groupby('date')['sales'].sum().reset_index()\n",
    "        \n",
    "        # Define spike as sales > 95th percentile\n",
    "        threshold = family_daily['sales'].quantile(0.95)\n",
    "        spike_days = family_daily[family_daily['sales'] > threshold]\n",
    "        normal_days = family_daily[family_daily['sales'] <= threshold]\n",
    "        \n",
    "        if len(spike_days) > 0:\n",
    "            spike_avg = spike_days['sales'].mean()\n",
    "            normal_avg = normal_days['sales'].mean()\n",
    "            spike_lift = (spike_avg - normal_avg) / normal_avg * 100\n",
    "            \n",
    "            print(f\"\\n{family}:\")\n",
    "            print(f\"  Normal days avg sales: ${normal_avg:,.2f}\")\n",
    "            print(f\"  Spike days avg sales: ${spike_avg:,.2f}\")\n",
    "            print(f\"  Spike lift: {spike_lift:.1f}%\")\n",
    "            print(f\"  Spike days count: {len(spike_days)} ({len(spike_days)/len(family_daily)*100:.1f}% of days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Key Findings:\n",
    "\n",
    "**Data Quality:**\n",
    "- Dataset spans multiple years with consistent daily records\n",
    "- First day of a year always has 0 sales \n",
    "\n",
    "**Sales Patterns:**\n",
    "- Clear growth trend over time\n",
    "- Seasonal patterns visible in monthly aggregations\n",
    "- High variability between stores and product families\n",
    "\n",
    "**Store Performance:**\n",
    "- Significant performance differences across store types and locations\n",
    "- Store clusters show distinct sales patterns\n",
    "- Geographic concentration affects performance\n",
    "\n",
    "**Product Insights:**\n",
    "- Top product families dominate total sales\n",
    "- Different families show varying seasonality and growth rates\n",
    "- Some categories are more volatile than others\n",
    "\n",
    "### Modeling Implications:\n",
    "- Consider store-specific models or clustering\n",
    "- Include external factors (oil prices, holidays)\n",
    "- Account for seasonality and trends\n",
    "- Handle negative sales appropriately\n",
    "- Feature engineering for location and store characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

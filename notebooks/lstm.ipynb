{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Clean LSTM Implementation for Store Sales Forecasting\n",
    "\n",
    "A properly designed Long Short-Term Memory (LSTM) network for time series forecasting with correct data handling, sequence preparation, and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "import joblib\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from evaluation.metrics import summary\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../content/data_processed')\n",
    "RESULTS_DIR = Path('../results/neural_networks/lstm')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and scalers\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load the datasets created by 02_data_preprocessing.ipynb\n",
    "train_df = pd.read_parquet(DATA_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(DATA_DIR / 'val.parquet') \n",
    "test_df = pd.read_parquet(DATA_DIR / 'test.parquet')\n",
    "\n",
    "# Load fitted scalers\n",
    "scalers = joblib.load(DATA_DIR / 'scalers.pkl')\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Available features: {train_df.columns.tolist()}\")\n",
    "print(f\"Date range - Train: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"Date range - Val: {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "print(f\"Date range - Test: {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nTrain data info:\")\n",
    "train_df.info()\n",
    "print(f\"\\nSales statistics in train data:\")\n",
    "print(train_df['sales'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for LSTM\n",
    "CONFIG = {\n",
    "    # Model architecture\n",
    "    'sequence_length': 15,              \n",
    "    'hidden_size': 16,                 \n",
    "    'num_layers': 3,                 \n",
    "    'bidirectional': False,            \n",
    "    'dropout': 0.3,                    \n",
    "    'output_size': 1,                  \n",
    "    \n",
    "    # Training - Strong regularization\n",
    "    'learning_rate': 1e-4,             \n",
    "    'weight_decay': 1e-3,              \n",
    "    'batch_size': 256,                 \n",
    "    'epochs': 30,                      \n",
    "    'patience': 10,                    \n",
    "    'min_delta': 1e-4,                 \n",
    "    'gradient_clip': 0.5,              \n",
    "    \n",
    "    # Anti-overfitting techniques\n",
    "    'label_smoothing': 0.1,           \n",
    "    'noise_factor': 0.01,              \n",
    "    'validation_frequency': 1,        \n",
    "    \n",
    "    # Data\n",
    "    'target_column': 'sales',\n",
    "    \n",
    "    # Performance optimizations\n",
    "    'num_workers': 2,                  \n",
    "    'pin_memory': True,                \n",
    "    'persistent_workers': False,       \n",
    "    'compile_model': False,            \n",
    "    'mixed_precision': False,          \n",
    "    'eval_frequency': 1,               \n",
    "    'log_frequency': 25,               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the preprocessed data - debug\n",
    "target_col = CONFIG['target_column']\n",
    "\n",
    "print(f\"\\nTarget ({target_col}) statistics:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    if target_col in df.columns:\n",
    "        stats = df[target_col].describe()\n",
    "        print(f\"  {name}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}, std={stats['std']:.2f}\")\n",
    "        negative_pct = (df[target_col] < 0).mean() * 100\n",
    "        print(f\"    Negative values: {negative_pct:.2f}%\")\n",
    "\n",
    "# Display available columns\n",
    "print(f\"\\nAvailable columns ({len(train_df.columns)}):\")\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features from preprocessed data\n",
    "print(\"Preparing features from preprocessed data...\")\n",
    "\n",
    "exclude_cols = [target_col, 'date']\n",
    "all_feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Feature preparation:\")\n",
    "print(f\"  Total columns: {len(train_df.columns)}\")\n",
    "print(f\"  Excluded: {exclude_cols}\")\n",
    "print(f\"  Feature columns: {len(all_feature_cols)}\")\n",
    "print(f\"  Sample features: {all_feature_cols[:10]}{'...' if len(all_feature_cols) > 10 else ''}\")\n",
    "\n",
    "X_train = train_df[all_feature_cols].values.astype(np.float32)\n",
    "X_val = val_df[all_feature_cols].values.astype(np.float32)\n",
    "X_test = test_df[all_feature_cols].values.astype(np.float32)\n",
    "\n",
    "y_train = train_df[target_col].values.astype(np.float32)\n",
    "y_val = val_df[target_col].values.astype(np.float32)\n",
    "y_test = test_df[target_col].values.astype(np.float32)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Data scaling check\n",
    "print(f\"\\nData scaling check:\")\n",
    "print(f\"  Feature range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"  Feature mean: {X_train.mean():.3f}, std: {X_train.std():.3f}\")\n",
    "print(f\"  Target range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "print(f\"  Target mean: {y_train.mean():.3f}, std: {y_train.std():.3f}\")\n",
    "\n",
    "# Check if features are in reasonable range for LSTM\n",
    "feature_range = X_train.max() - X_train.min()\n",
    "if feature_range > 100 or X_train.std() > 10:\n",
    "    print(\"WARNING: Features may not be properly scaled\")\n",
    "    print(\"Large feature values can cause training instability\")\n",
    "else:\n",
    "    print(\"Features appear to be reasonably scaled\")\n",
    "\n",
    "# Check target distribution \n",
    "if y_train.std() > 1000:\n",
    "    print(\"WARNING: Target has very high variance\")\n",
    "    print(\"Consider log transformation or different loss function\")\n",
    "elif (y_train < 0).any():\n",
    "    print(\"INFO: Target contains negative values\")\n",
    "else:\n",
    "    print(\"Target distribution looks reasonable\")\n",
    "\n",
    "# Data quality check\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"  X_train - NaN: {np.isnan(X_train).sum()}, Inf: {np.isinf(X_train).sum()}\")\n",
    "print(f\"  y_train - NaN: {np.isnan(y_train).sum()}, Inf: {np.isinf(y_train).sum()}\")\n",
    "\n",
    "if np.isnan(X_train).any() or np.isnan(y_train).any():\n",
    "    print(\"ERROR: Data contains NaN values\")\n",
    "elif np.isinf(X_train).any() or np.isinf(y_train).any():\n",
    "    print(\"ERROR: Data contains Inf values\")\n",
    "else:\n",
    "    print(\"Data quality check passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Time Series Dataset for LSTM with Proper Grouping\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], \n",
    "                 target_col: str, seq_length: int = 7):\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        group_cols = ['store_nbr', 'family', 'type']\n",
    "        available_group_cols = [col for col in group_cols if col in df.columns]\n",
    "        \n",
    "        if not available_group_cols:\n",
    "            available_group_cols = ['store_nbr', 'family']  # fallback\n",
    "        \n",
    "        # Group by store, family, and type to maintain temporal continuity\n",
    "        for group_key, group in df.groupby(available_group_cols):\n",
    "            group = group.sort_values('date')\n",
    "            \n",
    "            # Only create sequences if we have enough data\n",
    "            if len(group) > seq_length:\n",
    "                features = group[feature_cols].values.astype(np.float32)\n",
    "                targets = group[target_col].values.astype(np.float32)\n",
    "                \n",
    "                # CORRECTED: Create sequences within this group\n",
    "                for i in range(len(group) - seq_length):\n",
    "                    seq_features = features[i:i+seq_length]\n",
    "                    seq_target = targets[i+seq_length]\n",
    "                    \n",
    "                    self.sequences.append(seq_features)\n",
    "                    self.targets.append(seq_target)\n",
    "        \n",
    "        if len(self.sequences) == 0:\n",
    "            raise ValueError(\"No sequences created! Check your data and sequence length.\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.sequences = torch.FloatTensor(np.array(self.sequences))\n",
    "        self.targets = torch.FloatTensor(np.array(self.targets))\n",
    "        \n",
    "        # Validate data\n",
    "        assert not torch.isnan(self.sequences).any(), \"Sequences contain NaN values\"\n",
    "        assert not torch.isnan(self.targets).any(), \"Targets contain NaN values\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "    \n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self.sequences.shape[2]\n",
    "    \n",
    "def prepare_lstm_data_correctly(train_df, val_df, test_df, feature_cols, target_col, seq_length=7):\n",
    "    \"\"\"Properly prepare data for LSTM with grouped sequences\"\"\"\n",
    "    \n",
    "    print(f\"Preparing LSTM data with proper grouping...\")\n",
    "    print(f\"  Sequence length: {seq_length}\")\n",
    "    print(f\"  Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"  Target column: {target_col}\")\n",
    "    \n",
    "    # Create datasets with proper grouping\n",
    "    train_dataset = TimeSeriesDataset(train_df, feature_cols, target_col, seq_length)\n",
    "    val_dataset = TimeSeriesDataset(val_df, feature_cols, target_col, seq_length)\n",
    "    test_dataset = TimeSeriesDataset(test_df, feature_cols, target_col, seq_length)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM datasets\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING GROUPED LSTM DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = prepare_lstm_data_correctly(\n",
    "    train_df, val_df, test_df, \n",
    "    feature_cols=all_feature_cols,\n",
    "    target_col=target_col, \n",
    "    seq_length=CONFIG['sequence_length']\n",
    ")\n",
    "\n",
    "print(f\"\\nSuccessfully created LSTM datasets!\")\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} sequences\")\n",
    "print(f\"  Validation: {len(val_dataset)} sequences\")\n",
    "print(f\"  Test: {len(val_dataset)} sequences\")\n",
    "print(f\"  Input dimension: {test_dataset.input_dim}\")\n",
    "print(f\"  Sequence length: {CONFIG['sequence_length']}\")\n",
    "\n",
    "    \n",
    "print(f\"\\nFinal dataset verification:\")\n",
    "print(f\"  Train sequences: {len(train_dataset)}\")\n",
    "print(f\"  Validation sequences: {len(val_dataset)}\")\n",
    "print(f\"  Test sequences: {len(test_dataset)}\")\n",
    "print(f\"  Input shape per sequence: {train_dataset.sequences.shape[1:]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_size: int = 16, num_layers: int = 1, \n",
    "                 bidirectional: bool = False, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Ensure minimum hidden size for stability\n",
    "        if hidden_size < 4:\n",
    "            raise ValueError(f\"hidden_size must be at least 4, got {hidden_size}\")\n",
    "        \n",
    "        # Input dropout for regularization\n",
    "        self.input_dropout = nn.Dropout(dropout * 0.5)\n",
    "        \n",
    "        # LSTM with recurrent dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layers with controlled regularization\n",
    "        fc_input_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "        fc_hidden_dim = max(hidden_size // 2, 4)  # Ensure minimum size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout * 0.7),  # Reduce dropout slightly\n",
    "            nn.Linear(fc_input_dim, fc_hidden_dim),\n",
    "            nn.LayerNorm(fc_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),  # Reduce final dropout\n",
    "            nn.Linear(fc_hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Conservative weight initialization to prevent exploding gradients\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                # Xavier initialization for input weights\n",
    "                nn.init.xavier_normal_(param, gain=0.5)\n",
    "            elif 'weight_hh' in name:\n",
    "                # Orthogonal initialization for recurrent weights\n",
    "                nn.init.orthogonal_(param, gain=0.5)\n",
    "            elif 'bias_ih' in name or 'bias_hh' in name:\n",
    "                # Initialize all LSTM biases\n",
    "                nn.init.zeros_(param)\n",
    "                # Set forget gate bias to 1 for both input-hidden and hidden-hidden\n",
    "                hidden_size = param.size(0) // 4\n",
    "                param.data[hidden_size:2*hidden_size].fill_(1.0)\n",
    "        \n",
    "        # Initialize linear layers conservatively\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight, gain=0.5)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x, add_noise=False):\n",
    "        # Input dropout\n",
    "        x = self.input_dropout(x)\n",
    "        \n",
    "        # Add noise during training for regularization\n",
    "        if add_noise and self.training:\n",
    "            noise = torch.randn_like(x) * 0.01\n",
    "            x = x + noise\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use the last output\n",
    "        final_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Final prediction with dropout\n",
    "        output = self.fc(final_out).squeeze(-1)\n",
    "        return output\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Ensure non-negative predictions for RMSLE calculation\n",
    "        pred_clamped = torch.clamp(predictions, min=0) + self.epsilon\n",
    "        \n",
    "        # Calculate RMSLE using log1p for numerical stability\n",
    "        log_pred = torch.log1p(pred_clamped)\n",
    "        log_true = torch.log1p(targets + self.epsilon)\n",
    "        \n",
    "        mse_log = torch.mean((log_pred - log_true) ** 2)\n",
    "        rmsle = torch.sqrt(mse_log)\n",
    "        \n",
    "        return rmsle\n",
    "\n",
    "\n",
    "class AntiOverfittingTrainer:\n",
    "    \"\"\"Enhanced trainer with strong anti-overfitting measures\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, \n",
    "                 config: dict, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize WandB\n",
    "        wandb.init(\n",
    "            project=\"store-sales-forecasting\",\n",
    "            name=f\"lstm-rmsle-{int(time.time())}\",\n",
    "            config=config,\n",
    "            tags=[\"lstm\", \"anti-overfitting\", \"rmsle-loss\", \"time-series\"],\n",
    "            reinit=True,\n",
    "        )\n",
    "        \n",
    "        # RMSLE loss\n",
    "        self.criterion = RMSLELoss(epsilon=1e-6)\n",
    "        print(\"Using RMSLE loss function for time series forecasting\")\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        # Aggressive learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            mode='min', \n",
    "            factor=0.3,  # More aggressive reduction\n",
    "            patience=2,  # Faster response\n",
    "            min_lr=1e-7,\n",
    "        )\n",
    "        \n",
    "        # Training state with validation tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.patience_counter = 0\n",
    "        self.val_metrics_history = []\n",
    "        \n",
    "        # Overfitting detection\n",
    "        self.overfitting_threshold = 1.5 \n",
    "        \n",
    "        print(f\"Anti-overfitting trainer initialized with RMSLE loss:\")\n",
    "        print(f\"  RMSLE epsilon: {1e-6}\")\n",
    "        print(f\"  Weight decay: {config['weight_decay']}\")\n",
    "        print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "        print(f\"  Early stopping patience: {config['patience']}\")\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Training epoch with noise injection\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (features, targets) in enumerate(self.train_loader):\n",
    "            features, targets = features.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Forward pass with noise injection\n",
    "            if hasattr(self.model, 'forward') and 'add_noise' in self.model.forward.__code__.co_varnames:\n",
    "                outputs = self.model(features, add_noise=True)\n",
    "            else:\n",
    "                outputs = self.model(features)\n",
    "            \n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if self.config.get('gradient_clip', 0) > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    self.config['gradient_clip']\n",
    "                )\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if batch_idx % 25 == 0:\n",
    "                print(f\"\\rBatch {batch_idx}/{len(self.train_loader)} - Loss: {loss.item():.6f}\", end=\"\")\n",
    "        \n",
    "        print()\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"Evaluation without noise\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in dataloader:\n",
    "                features, targets = features.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(features)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, np.array(predictions), np.array(actuals)\n",
    "    \n",
    "    def detect_overfitting(self, train_loss, val_loss, epoch):\n",
    "        \"\"\"Detect overfitting patterns\"\"\"\n",
    "        overfitting_detected = False\n",
    "        \n",
    "        if val_loss > train_loss * self.overfitting_threshold:\n",
    "            print(f\"Overfitting detected: val_loss ({val_loss:.6f}) > {self.overfitting_threshold}x train_loss ({train_loss:.6f})\")\n",
    "            overfitting_detected = True\n",
    "        \n",
    "        # Check if validation loss is trending upward\n",
    "        if len(self.val_losses) >= 3:\n",
    "            recent_val_losses = self.val_losses[-3:]\n",
    "            if all(recent_val_losses[i] <= recent_val_losses[i+1] for i in range(len(recent_val_losses)-1)):\n",
    "                print(f\"Validation loss trending upward for 3 epochs\")\n",
    "                overfitting_detected = True\n",
    "        \n",
    "        return overfitting_detected\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Starting LSTM training with RMSLE loss...\")\n",
    "        print(f\"Model parameters: {self.model.get_num_parameters():,}\")\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_preds, val_actuals = self.evaluate(self.val_loader)\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_metrics = summary(val_actuals, val_preds) if val_preds.std() > 1e-8 else {'MAE': float('inf'), 'RMSE': float('inf'), 'RMSLE': float('inf'), 'MAPE': float('inf'), 'SMAPE': float('inf')}\n",
    "            self.val_metrics_history.append(val_metrics)\n",
    "            \n",
    "            # Detect overfitting\n",
    "            overfitting = self.detect_overfitting(train_loss, val_loss, epoch)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping and best model saving\n",
    "            if val_loss < self.best_val_loss - self.config['min_delta']:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.patience_counter = 0\n",
    "                self.save_checkpoint(epoch, val_metrics)\n",
    "                status = \"Best\"\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                status = f\" {self.patience_counter}/{self.config['patience']}\"\n",
    "            \n",
    "            # Calculate logging variables\n",
    "            epoch_time = time.time() - start_time\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Log to WandB\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_rmsle_loss': train_loss,\n",
    "                'val_rmsle_loss': val_loss,\n",
    "                'val_mae': val_metrics['MAE'],\n",
    "                'val_rmse': val_metrics['RMSE'],\n",
    "                'val_rmsle': val_metrics['RMSLE'],\n",
    "                'val_mape': val_metrics['MAPE'],\n",
    "                'val_smape': val_metrics['SMAPE'],\n",
    "                'learning_rate': lr,\n",
    "                'epoch_time': epoch_time,\n",
    "                'patience_counter': self.patience_counter\n",
    "            })\n",
    "            \n",
    "            # Display progress\n",
    "            print(f\"Epoch {epoch+1:3d}/{self.config['epochs']} | \"\n",
    "                  f\"Train RMSLE: {train_loss:.6f} | Val RMSLE: {val_loss:.6f} | \"\n",
    "                  f\"Ratio: {val_loss/train_loss:.2f} | \"\n",
    "                  f\"Val RMSLE Metric: {val_metrics['RMSLE']:.6f} | \"\n",
    "                  f\"LR: {lr:.2e} | Time: {epoch_time:.1f}s | {status}\")\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.config['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            # Force stop on severe overfitting\n",
    "            if overfitting and epoch > 5:\n",
    "                print(f\"Stopping due to overfitting detection at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"\\nLoaded best model with validation loss: {self.best_val_loss:.6f}\")\n",
    "        \n",
    "        wandb.finish()\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_metrics):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': self.best_val_loss,\n",
    "            'val_metrics': val_metrics,\n",
    "            'config': self.config,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "        torch.save(checkpoint, RESULTS_DIR / 'anti_overfitting_model.pt')\n",
    "    \n",
    "    def final_evaluation(self):\n",
    "        \"\"\"Final evaluation on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"FINAL ANTI-OVERFITTING EVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        test_loss, test_preds, test_actuals = self.evaluate(self.test_loader)\n",
    "        test_metrics = summary(test_actuals, test_preds)\n",
    "        \n",
    "        print(f\"Test RMSLE Loss: {test_loss:.6f}\")\n",
    "        print(\"\\nTest Metrics:\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"  {metric}: {value:.6f}\")\n",
    "        \n",
    "        # Overfitting analysis\n",
    "        final_train_loss = self.train_losses[-1] if self.train_losses else float('inf')\n",
    "        final_val_loss = self.val_losses[-1] if self.val_losses else float('inf')\n",
    "        \n",
    "        print(f\"\\nOverfitting Analysis:\")\n",
    "        print(f\"  Final train RMSLE: {final_train_loss:.6f}\")\n",
    "        print(f\"  Final val RMSLE: {final_val_loss:.6f}\")\n",
    "        print(f\"  Test RMSLE: {test_loss:.6f}\")\n",
    "        print(f\"  Val/Train ratio: {final_val_loss/final_train_loss:.2f}\")\n",
    "        print(f\"  Test/Train ratio: {test_loss/final_train_loss:.2f}\")\n",
    "        \n",
    "        return test_metrics, test_preds, test_actuals\n",
    "\n",
    "\n",
    "# Keep the original Trainer class for compatibility\n",
    "class Trainer(AntiOverfittingTrainer):\n",
    "    pass\n",
    "\n",
    "print(\"Updated trainer with anti-overfitting measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create proper LSTM datasets using the preprocessed data\n",
    "print(\"Creating LSTM datasets from preprocessed data...\")\n",
    "\n",
    "\n",
    "print(f\"Successfully created LSTM datasets!\")\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} sequences\")\n",
    "print(f\"  Validation: {len(val_dataset)} sequences\")\n",
    "print(f\"  Test: {len(test_dataset)} sequences\")\n",
    "print(f\"  Input dimension: {train_dataset.input_dim}\")\n",
    "print(f\"  Sequence length: {CONFIG['sequence_length']}\")\n",
    "\n",
    "# Create OPTIMIZED data loaders with the corrected datasets\n",
    "print(\"Creating optimized data loaders with properly grouped sequences...\")\n",
    "\n",
    "# Verify we have datasets\n",
    "if 'train_dataset' not in locals():\n",
    "    print(\"❌ Error: No datasets found. Please run the dataset creation cell first.\")\n",
    "    raise ValueError(\"Datasets not created. Run the previous cells first.\")\n",
    "\n",
    "print(f\"Using datasets:\")\n",
    "print(f\"  Train: {len(train_dataset)} sequences\")\n",
    "print(f\"  Validation: {len(val_dataset)} sequences\")\n",
    "print(f\"  Test: {len(test_dataset)} sequences\")\n",
    "print(f\"  Input dimension: {train_dataset.input_dim}\")\n",
    "print(f\"  Sequence length: {CONFIG['sequence_length']}\")\n",
    "\n",
    "# Create OPTIMIZED data loaders with performance settings\n",
    "is_cuda = device.type == 'cuda'\n",
    "num_workers = CONFIG.get('num_workers', 4) if is_cuda else 0 \n",
    "pin_memory = CONFIG.get('pin_memory', True) and is_cuda\n",
    "persistent_workers = CONFIG.get('persistent_workers', True) and num_workers > 0\n",
    "\n",
    "print(f\"\\nData loader settings:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Num workers: {num_workers}\")\n",
    "print(f\"  Pin memory: {pin_memory}\")\n",
    "print(f\"  Persistent workers: {persistent_workers}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers,\n",
    "    drop_last=True,  # For consistent batch sizes\n",
    "    prefetch_factor=2 if num_workers > 0 else None\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers,\n",
    "    drop_last=False,\n",
    "    prefetch_factor=2 if num_workers > 0 else None\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    persistent_workers=persistent_workers,\n",
    "    drop_last=False,\n",
    "    prefetch_factor=2 if num_workers > 0 else None\n",
    ")\n",
    "\n",
    "print(\"\\n Data loaders created successfully with proper grouping!\")\n",
    "print(f\"Data loader batch counts:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Validation: {len(val_loader)} batches\") \n",
    "print(f\"  Test: {len(test_loader)} batches\")\n",
    "\n",
    "# Test a batch to ensure everything works\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"\\nSample batch test successful:\")\n",
    "    print(f\"  Batch input shape: {sample_batch[0].shape}\")\n",
    "    print(f\"  Batch target shape: {sample_batch[1].shape}\")\n",
    "    print(f\"  Expected: (batch_size={CONFIG['batch_size']}, seq_len={CONFIG['sequence_length']}, features={train_dataset.input_dim})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in sample batch: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularized model to prevent overfitting\n",
    "model = LSTMModel(\n",
    "    input_dim=train_dataset.input_dim,\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    bidirectional=CONFIG['bidirectional'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "print(\"Anti-Overfitting Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"Total parameters: {model.get_num_parameters():,}\")\n",
    "print(f\"Model size reduction: {model.get_num_parameters()} vs previous larger models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING ANTI-OVERFITTING LSTM TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer = AntiOverfittingTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    config=CONFIG,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "start_training_time = time.time()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    total_training_time = time.time() - start_training_time\n",
    "    print(f\"\\nTraining completed in {total_training_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Best validation loss: {trainer.best_val_loss:.6f}\")\n",
    "    print(f\"  Final train/val loss ratio: {trainer.val_losses[-1]/trainer.train_losses[-1]:.2f}\")\n",
    "    print(f\"  Early stopping triggered: {trainer.patience_counter >= CONFIG['patience']}\")\n",
    "    print(f\"  Epochs completed: {len(trainer.train_losses)}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "    total_training_time = time.time() - start_training_time\n",
    "    print(f\"Training time before interruption: {total_training_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed with error: {e}\")\n",
    "    total_training_time = time.time() - start_training_time\n",
    "    print(f\"Training time before failure: {total_training_time:.2f} seconds\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "test_metrics, test_preds, test_actuals = trainer.final_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Training curves\n",
    "axes[0, 0].plot(trainer.train_losses, label='Training Loss', alpha=0.8)\n",
    "axes[0, 0].plot(trainer.val_losses, label='Validation Loss', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].set_title('Training Progress')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions vs Actuals\n",
    "axes[0, 1].scatter(test_actuals, test_preds, alpha=0.6, s=1)\n",
    "min_val, max_val = min(test_actuals.min(), test_preds.min()), max(test_actuals.max(), test_preds.max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Sales')\n",
    "axes[0, 1].set_ylabel('Predicted Sales')\n",
    "axes[0, 1].set_title('Predictions vs Actuals')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals\n",
    "residuals = test_actuals - test_preds\n",
    "axes[1, 0].scatter(test_preds, residuals, alpha=0.6, s=1)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 0].set_xlabel('Predicted Sales')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residuals Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Error distribution\n",
    "axes[1, 1].hist(residuals, bins=50, alpha=0.7, density=True)\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].set_xlabel('Residuals')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Residuals Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'lstm_evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'lstm_evaluation_plots.png'}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nFINAL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: LSTM with {model.get_num_parameters():,} parameters\")\n",
    "print(f\"Architecture: {CONFIG['hidden_size']} hidden units, {CONFIG['num_layers']} layers\")\n",
    "print(f\"Bidirectional: {CONFIG['bidirectional']}\")\n",
    "print(f\"\")\n",
    "print(f\"Test Metrics (Original Scale):\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric:6s}: {value:10.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"Prediction Quality:\")\n",
    "print(f\"  Actual range:    [{test_actuals.min():8.2f}, {test_actuals.max():8.2f}]\")\n",
    "print(f\"  Predicted range: [{test_preds.min():8.2f}, {test_preds.max():8.2f}]\")\n",
    "print(f\"  Mean error:      {residuals.mean():8.2f}\")\n",
    "print(f\"  Std error:       {residuals.std():8.2f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

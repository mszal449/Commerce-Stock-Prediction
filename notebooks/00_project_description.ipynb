{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Store Sales Time Series Forecasting with Neural Networks\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project investigates the application of neural networks for demand forecasting in retail environments. The primary objective is to answer whether neural models – from simple dense networks (MLP) to more advanced architectures like LSTM – can effectively predict product demand and thereby support optimization of supply chain costs and inventory management.\n",
    "\n",
    "**Key Research Question**: Can neural networks outperform traditional time series forecasting methods in predicting retail sales, and what is the optimal architecture for this specific domain?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Project Objectives\n",
    "\n",
    "### Primary Objectives\n",
    "1. **Develop and Compare Neural Network Architectures** for time series forecasting in retail sales\n",
    "2. **Evaluate Performance** against traditional baseline methods (Linear Regression, Prophet)\n",
    "3. **Optimize Model Architecture** for different product categories and store types\n",
    "4. **Provide Practical Insights** for inventory management and demand planning\n",
    "\n",
    "### Secondary Objectives\n",
    "1. **Feature Engineering**: Identify and create relevant features from temporal, promotional, and external data\n",
    "2. **Model Interpretability**: Understand what patterns neural networks learn from sales data\n",
    "3. **Scalability Assessment**: Evaluate computational requirements for real-world deployment\n",
    "4. **Cross-validation Strategy**: Develop appropriate time series validation methodology\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "### Source\n",
    "**Kaggle Competition**: [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data)\n",
    "\n",
    "**Corporación Favorita Grocery Sales Dataset**\n",
    "- **Time Period**: 2013-01-01 to 2017-08-31 (1,684 days)\n",
    "- **Stores**: 54 retail stores across Ecuador\n",
    "- **Product Families**: 33 different product categories\n",
    "- **Total Records**: ~3 million daily sales records\n",
    "- **Coverage**: Complete grid format (store × product × date)\n",
    "\n",
    "### Data Files\n",
    "1. **train.csv** - Historical sales data\n",
    "   - `date`: Date of sale\n",
    "   - `store_nbr`: Store identifier (1-54)\n",
    "   - `family`: Product family/category\n",
    "   - `sales`: Units sold (target variable)\n",
    "   - `onpromotion`: Number of items on promotion\n",
    "\n",
    "2. **test.csv** - Test set for predictions (15 days after training period)\n",
    "\n",
    "3. **stores.csv** - Store metadata\n",
    "   - `store_nbr`: Store identifier\n",
    "   - `city`: Store location\n",
    "   - `state`: State/province\n",
    "   - `type`: Store type (A, B, C, D, E)\n",
    "   - `cluster`: Store cluster (1-17)\n",
    "\n",
    "4. **oil.csv** - Daily oil prices (Ecuador's economy is oil-dependent)\n",
    "   - `date`: Date\n",
    "   - `dcoilwtico`: Oil price\n",
    "\n",
    "5. **holidays_events.csv** - Holiday and event information\n",
    "   - `date`: Date\n",
    "   - `type`: Holiday type (Holiday, Event, etc.)\n",
    "   - `locale`: Geographic scope (National, Regional, Local)\n",
    "   - `transferred`: Whether holiday was transferred\n",
    "\n",
    "6. **transactions.csv** - Daily transaction counts by store\n",
    "   - `date`: Date\n",
    "   - `store_nbr`: Store identifier\n",
    "   - `transactions`: Number of transactions\n",
    "\n",
    "### Data Characteristics\n",
    "- **Temporal Granularity**: Daily sales records\n",
    "- **Missing Data**: Zero sales records (~31% of dataset)\n",
    "- **Sales Range**: $0.01 to $3,502 per day per store-product\n",
    "- **Seasonality**: Clear weekly and monthly patterns\n",
    "- **External Factors**: Oil prices, holidays, promotions impact sales\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Neural Network Architectures\n",
    "\n",
    "### 1. Baseline Models\n",
    "- **Linear Regression**: Simple linear relationship modeling\n",
    "- **Prophet**: Facebook's time series forecasting tool\n",
    "- **Moving Averages**: Simple and exponential smoothing\n",
    "\n",
    "### 2. Multi-Layer Perceptron (MLP)\n",
    "- **Architecture**: Dense feedforward network\n",
    "- **Input**: Windowed time series features + categorical embeddings\n",
    "- **Hidden Layers**: 2-4 layers with 128-512 neurons\n",
    "- **Activation**: ReLU for hidden layers, Linear for output\n",
    "- **Regularization**: Dropout (0.2-0.5), L2 regularization\n",
    "\n",
    "### 3. Long Short-Term Memory (LSTM)\n",
    "- **Architecture**: Recurrent neural network with memory cells\n",
    "- **Input**: Sequential time series data (lookback window)\n",
    "- **LSTM Layers**: 1-3 layers with 64-256 units\n",
    "- **Output**: Dense layer for final prediction\n",
    "- **Variants**: Vanilla LSTM, Bidirectional LSTM, Stacked LSTM\n",
    "\n",
    "### 4. Advanced Architectures (Optional)\n",
    "- **CNN-LSTM**: Convolutional layers for feature extraction + LSTM for temporal modeling\n",
    "- **Transformer**: Attention-based architecture for sequence modeling\n",
    "- **GRU**: Gated Recurrent Unit as LSTM alternative\n",
    "\n",
    "### 5. TabPFN (Tabular Prior-Data Fitted Networks)\n",
    "- **Architecture**: Pre-trained Transformer for small tabular datasets\n",
    "- **Approach**: Prior-Data Fitted Networks using synthetic datasets\n",
    "- **Advantages**: \n",
    "  - No hyperparameter tuning required\n",
    "  - Extremely fast inference (seconds)\n",
    "  - Strong performance on small tabular problems\n",
    "  - No training needed - uses pre-trained weights\n",
    "- **Application**: Feature-based forecasting (convert time series to tabular format)\n",
    "- **Limitations**: Maximum 100 features, 10,000 samples per prediction\n",
    "- **Use Case**: Benchmark against traditional tabular ML approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data Input Specifications\n",
    "\n",
    "### Feature Engineering Pipeline\n",
    "\n",
    "#### 1. Temporal Features\n",
    "- **Lag Features**: Sales from previous 1, 7, 14, 30 days\n",
    "- **Rolling Statistics**: Mean, std, min, max over 7, 14, 30-day windows\n",
    "- **Seasonal Features**: Day of week, month, quarter, year\n",
    "- **Calendar Features**: Is weekend, is month start/end, days from holiday\n",
    "\n",
    "#### 2. Categorical Features\n",
    "- **Store Information**: Store number, type, cluster, city, state\n",
    "- **Product Information**: Product family\n",
    "- **Encoding Method**: Embedding layers for neural networks\n",
    "\n",
    "#### 3. External Features\n",
    "- **Oil Prices**: Current and lagged oil prices\n",
    "- **Holidays**: Holiday indicators (national, regional, local)\n",
    "- **Promotions**: Number of items on promotion\n",
    "- **Transactions**: Store transaction counts\n",
    "\n",
    "### Input Data Format\n",
    "\n",
    "#### For MLP Models\n",
    "```python\n",
    "# Feature vector per sample\n",
    "Input Shape: (batch_size, n_features)\n",
    "\n",
    "# Example feature vector:\n",
    "[\n",
    "    # Temporal features (10)\n",
    "    sales_lag_1, sales_lag_7, sales_lag_14, sales_lag_30,\n",
    "    rolling_mean_7, rolling_std_7, rolling_mean_30, rolling_std_30,\n",
    "    day_of_week, month,\n",
    "    \n",
    "    # Store embeddings (embedded to 8 dimensions)\n",
    "    store_embedding_1, store_embedding_2, ..., store_embedding_8,\n",
    "    \n",
    "    # Product embeddings (embedded to 6 dimensions)\n",
    "    family_embedding_1, family_embedding_2, ..., family_embedding_6,\n",
    "    \n",
    "    # External features (4)\n",
    "    oil_price, is_holiday, onpromotion, transactions\n",
    "]\n",
    "\n",
    "Total Features: ~28 dimensions\n",
    "```\n",
    "\n",
    "#### For LSTM Models\n",
    "```python\n",
    "# Sequential data with lookback window\n",
    "Input Shape: (batch_size, sequence_length, n_features)\n",
    "\n",
    "# Example: 30-day lookback window\n",
    "sequence_length = 30\n",
    "n_features = 8  # [sales, oil_price, onpromotion, transactions, \n",
    "                #  day_of_week, month, is_holiday, is_weekend]\n",
    "\n",
    "# Each sample contains 30 consecutive days of data\n",
    "# Target: sales value for day 31\n",
    "```\n",
    "\n",
    "#### For TabPFN Models\n",
    "```python\n",
    "# Tabular format with engineered features\n",
    "Input Shape: (batch_size, n_features)  # Max 100 features, 10,000 samples\n",
    "\n",
    "# Example feature vector for TabPFN:\n",
    "[\n",
    "    # Statistical features from last 7/14/30 days (60 features)\n",
    "    sales_mean_7d, sales_std_7d, sales_min_7d, sales_max_7d, sales_last_7d,\n",
    "    sales_mean_14d, sales_std_14d, sales_min_14d, sales_max_14d, sales_last_14d,\n",
    "    sales_mean_30d, sales_std_30d, sales_min_30d, sales_max_30d, sales_last_30d,\n",
    "    # ... similar for oil_price, onpromotion, transactions (45 more features)\n",
    "    \n",
    "    # Categorical features (one-hot encoded, 35 features)\n",
    "    store_type_A, store_type_B, store_type_C, store_type_D, store_type_E,\n",
    "    family_automotive, family_baby_care, ..., family_other,  # 33 families\n",
    "    is_weekend,\n",
    "    \n",
    "    # Total: ~100 features (within TabPFN limit)\n",
    "]\n",
    "\n",
    "# Note: Time series is converted to cross-sectional tabular format\n",
    "# Each row represents one store-product-date combination\n",
    "# Features capture temporal patterns through statistical aggregations\n",
    "```\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "\n",
    "1. **Handling Zero Sales**\n",
    "   - Option 1: Log transformation with offset: log(sales + 1)\n",
    "   - Option 2: Separate binary classifier for zero vs non-zero\n",
    "   - Option 3: Focus on positive sales only\n",
    "\n",
    "2. **Normalization**\n",
    "   - Numerical features: StandardScaler or MinMaxScaler\n",
    "   - Sales target: Log transformation or standardization\n",
    "\n",
    "3. **Time Series Split**\n",
    "   - Training: 2013-01-01 to 2017-07-31\n",
    "   - Validation: 2017-08-01 to 2017-08-15\n",
    "   - Test: 2017-08-16 to 2017-08-31\n",
    "\n",
    "4. **Cross-Validation Strategy**\n",
    "   - Time Series Split: Expanding window validation\n",
    "   - No random shuffling (preserves temporal order)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Output Format Specifications\n",
    "\n",
    "### Model Predictions\n",
    "\n",
    "#### Single-Step Forecasting\n",
    "```python\n",
    "# Output for each model prediction\n",
    "Output Shape: (n_samples, 1)\n",
    "\n",
    "# Example output:\n",
    "predictions = [\n",
    "    [12.45],    # Predicted sales for store 1, family A, date t+1\n",
    "    [8.73],     # Predicted sales for store 1, family B, date t+1\n",
    "    [23.12],    # Predicted sales for store 2, family A, date t+1\n",
    "    ...\n",
    "]\n",
    "\n",
    "# Post-processing:\n",
    "# - Ensure non-negative predictions: max(0, prediction)\n",
    "# - Inverse transform if log scaling was applied\n",
    "# - Round to appropriate precision (0.01 for currency)\n",
    "```\n",
    "\n",
    "#### Multi-Step Forecasting (Optional)\n",
    "```python\n",
    "# Output for multiple days ahead\n",
    "Output Shape: (n_samples, forecast_horizon)\n",
    "\n",
    "# Example: 15-day forecast\n",
    "forecasts = [\n",
    "    [12.45, 11.23, 13.67, ..., 14.56],  # 15-day forecast for sample 1\n",
    "    [8.73, 9.12, 8.45, ..., 9.23],      # 15-day forecast for sample 2\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Submission Format\n",
    "```python\n",
    "# Kaggle competition format\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [f'{date}_{store_nbr}_{family}' for ...],  # Unique identifier\n",
    "    'sales': predictions  # Predicted sales values\n",
    "})\n",
    "\n",
    "# Example:\n",
    "#           id                    sales\n",
    "# 0   2017-08-16_1_AUTOMOTIVE     12.45\n",
    "# 1   2017-08-16_1_BABY_CARE       8.73\n",
    "# 2   2017-08-16_1_BEAUTY         23.12\n",
    "# ...\n",
    "```\n",
    "\n",
    "### Model Outputs Structure\n",
    "```python\n",
    "# Comprehensive model output dictionary\n",
    "model_output = {\n",
    "    'predictions': predictions,           # Raw predictions\n",
    "    'confidence_intervals': intervals,    # Prediction intervals (if available)\n",
    "    'feature_importance': importance,     # Feature importance scores\n",
    "    'training_history': history,          # Training loss/metrics history\n",
    "    'validation_metrics': val_metrics,    # Validation performance\n",
    "    'model_metadata': {\n",
    "        'architecture': 'LSTM',\n",
    "        'parameters': model_params,\n",
    "        'training_time': training_time,\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "### Primary Metrics\n",
    "\n",
    "#### 1. Root Mean Square Logarithmic Error (RMSLE)\n",
    "```python\n",
    "# Primary metric for Kaggle competition\n",
    "RMSLE = sqrt(mean((log(predicted + 1) - log(actual + 1))^2))\n",
    "\n",
    "# Advantages:\n",
    "# - Penalizes underestimation more than overestimation\n",
    "# - Handles zero values well\n",
    "# - Scale-invariant\n",
    "```\n",
    "\n",
    "#### 2. Mean Absolute Error (MAE)\n",
    "```python\n",
    "# Interpretable metric in original units\n",
    "MAE = mean(|predicted - actual|)\n",
    "\n",
    "# Advantages:\n",
    "# - Easy to interpret (average prediction error)\n",
    "# - Robust to outliers\n",
    "# - Same units as target variable\n",
    "```\n",
    "\n",
    "#### 3. Root Mean Square Error (RMSE)\n",
    "```python\n",
    "# Standard regression metric\n",
    "RMSE = sqrt(mean((predicted - actual)^2))\n",
    "\n",
    "# Advantages:\n",
    "# - Penalizes large errors more heavily\n",
    "# - Commonly used benchmark\n",
    "# - Same units as target variable\n",
    "```\n",
    "\n",
    "### Secondary Metrics\n",
    "\n",
    "#### 4. Mean Absolute Percentage Error (MAPE)\n",
    "```python\n",
    "# Percentage-based metric\n",
    "MAPE = mean(|actual - predicted| / |actual|) * 100\n",
    "\n",
    "# Advantages:\n",
    "# - Scale-independent\n",
    "# - Easy to interpret as percentage\n",
    "# - Good for business understanding\n",
    "```\n",
    "\n",
    "#### 5. Weighted MAPE (wMAPE)\n",
    "```python\n",
    "# Weighted version to handle zero sales\n",
    "wMAPE = sum(|actual - predicted|) / sum(|actual|) * 100\n",
    "\n",
    "# Advantages:\n",
    "# - Better handling of zero/small values\n",
    "# - More stable than standard MAPE\n",
    "```\n",
    "\n",
    "#### 6. Directional Accuracy\n",
    "```python\n",
    "# Percentage of correct trend predictions\n",
    "DA = mean(sign(actual[t] - actual[t-1]) == sign(predicted[t] - actual[t-1]))\n",
    "\n",
    "# Advantages:\n",
    "# - Measures trend prediction accuracy\n",
    "# - Important for inventory planning\n",
    "```\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "#### Time Series Cross-Validation\n",
    "```python\n",
    "# Expanding window validation\n",
    "for fold in range(n_folds):\n",
    "    train_end = initial_train_size + fold * step_size\n",
    "    val_start = train_end + 1\n",
    "    val_end = val_start + validation_window\n",
    "    \n",
    "    # Train on expanding window\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[val_start:val_end]\n",
    "    \n",
    "    # Evaluate and store metrics\n",
    "    metrics[fold] = evaluate_model(model, train_data, val_data)\n",
    "```\n",
    "\n",
    "#### Evaluation by Segments\n",
    "1. **By Product Family**: Performance for each of 33 product categories\n",
    "2. **By Store Type**: Performance across different store types (A, B, C, D, E)\n",
    "3. **By Store Cluster**: Performance across 17 store clusters\n",
    "4. **By Time Period**: Performance during different seasons/months\n",
    "5. **By Sales Volume**: Performance for high vs low-volume products\n",
    "\n",
    "#### Statistical Significance Testing\n",
    "```python\n",
    "# Diebold-Mariano test for forecast accuracy comparison\n",
    "from scipy import stats\n",
    "\n",
    "# Test if Model A significantly outperforms Model B\n",
    "dm_statistic, p_value = diebold_mariano_test(errors_A, errors_B)\n",
    "print(f\"DM statistic: {dm_statistic:.4f}, p-value: {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "\n",
    "### Model Training Pipeline\n",
    "\n",
    "#### 1. Data Preparation\n",
    "```python\n",
    "# Pipeline steps\n",
    "1. Load raw data files\n",
    "2. Feature engineering (lags, rolling stats, embeddings)\n",
    "3. Handle zero sales (transformation or separate modeling)\n",
    "4. Split data chronologically\n",
    "5. Scale/normalize features\n",
    "6. Create sequences for LSTM models\n",
    "```\n",
    "\n",
    "#### 2. Hyperparameter Optimization\n",
    "```python\n",
    "# Grid search parameters\n",
    "mlp_params = {\n",
    "    'hidden_layers': [2, 3, 4],\n",
    "    'hidden_units': [128, 256, 512],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [64, 128, 256]\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'lstm_units': [64, 128, 256],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'sequence_length': [14, 30, 60],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "tabpfn_params = {\n",
    "    'feature_window': [7, 14, 30],  # Number of days to create features from\n",
    "    'aggregation_methods': ['mean', 'std', 'min', 'max', 'last'],\n",
    "    'max_features': 100,  # TabPFN limitation\n",
    "    'ensemble_size': [1, 4, 16],  # Number of ensemble members\n",
    "    'preprocessing': ['standard', 'none']  # Feature scaling options\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. Training Configuration\n",
    "```python\n",
    "# Training settings\n",
    "training_config = {\n",
    "    'epochs': 100,\n",
    "    'early_stopping': {\n",
    "        'patience': 10,\n",
    "        'monitor': 'val_loss',\n",
    "        'restore_best_weights': True\n",
    "    },\n",
    "    'callbacks': [\n",
    "        'EarlyStopping',\n",
    "        'ReduceLROnPlateau',\n",
    "        'ModelCheckpoint'\n",
    "    ],\n",
    "    'validation_split': 0.2  # From training data\n",
    "}\n",
    "```\n",
    "\n",
    "### Implementation Checklist\n",
    "\n",
    "#### Phase 1: Data Exploration ✅\n",
    "- [x] Load and explore dataset\n",
    "- [x] Analyze sales patterns and distributions\n",
    "- [x] Identify data quality issues\n",
    "- [x] Visualize temporal patterns\n",
    "- [x] Analyze store and product performance\n",
    "\n",
    "#### Phase 2: Data Preprocessing\n",
    "- [ ] Implement feature engineering pipeline\n",
    "- [ ] Handle zero sales appropriately\n",
    "- [ ] Create categorical embeddings\n",
    "- [ ] Implement time series data splitting\n",
    "- [ ] Create data loaders for different model types\n",
    "\n",
    "#### Phase 3: Baseline Models\n",
    "- [ ] Implement linear regression baseline\n",
    "- [ ] Implement Prophet model\n",
    "- [ ] Implement moving averages\n",
    "- [ ] Establish baseline performance metrics\n",
    "\n",
    "#### Phase 4: Neural Network Implementation\n",
    "- [ ] Implement MLP architecture\n",
    "- [ ] Implement LSTM architecture\n",
    "- [ ] Implement TabPFN approach (feature engineering + pre-trained model)\n",
    "- [ ] Hyperparameter tuning\n",
    "- [ ] Model training and validation\n",
    "\n",
    "#### Phase 5: Model Comparison and Analysis\n",
    "- [ ] Compare all models using established metrics\n",
    "- [ ] Statistical significance testing\n",
    "- [ ] Error analysis by segments\n",
    "- [ ] Feature importance analysis\n",
    "- [ ] Computational efficiency comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Expected Outcomes\n",
    "\n",
    "### Research Questions to Answer\n",
    "\n",
    "1. **Model Performance Hierarchy**\n",
    "   - Do neural networks outperform traditional methods?\n",
    "   - Which architecture performs best for different product categories?\n",
    "   - How does performance vary across stores and time periods?\n",
    "   - Can TabPFN compete with specialized time series methods?\n",
    "\n",
    "2. **Feature Importance**\n",
    "   - Which features are most predictive of sales?\n",
    "   - How important are external factors (oil prices, holidays)?\n",
    "   - Do categorical embeddings improve performance?\n",
    "   - What feature engineering works best for TabPFN?\n",
    "\n",
    "3. **Practical Considerations**\n",
    "   - What is the computational cost vs accuracy trade-off?\n",
    "   - How stable are the models across different time periods?\n",
    "   - Can the models handle seasonal variations effectively?\n",
    "   - Is TabPFN's zero-shot approach viable for retail forecasting?\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "#### Quantitative Targets\n",
    "- **RMSLE < 0.5**: Competitive performance on Kaggle leaderboard\n",
    "- **MAE < $5**: Practical accuracy for inventory planning\n",
    "- **MAPE < 15%**: Industry-standard forecasting accuracy\n",
    "- **Neural Network Improvement**: >10% improvement over baselines\n",
    "\n",
    "#### Qualitative Targets\n",
    "- Clear understanding of when neural networks excel vs traditional methods\n",
    "- Actionable insights for retail demand forecasting\n",
    "- Robust model that generalizes across different product categories\n",
    "- Interpretable results that can guide business decisions\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "1. **Jupyter Notebooks**\n",
    "   - Data exploration and analysis\n",
    "   - Model implementation and training\n",
    "   - Results comparison and visualization\n",
    "\n",
    "2. **Source Code**\n",
    "   - Modular, reusable implementations\n",
    "   - Data preprocessing pipeline\n",
    "   - Model training and evaluation scripts\n",
    "\n",
    "3. **Technical Report**\n",
    "   - Methodology and experimental design\n",
    "   - Results analysis and interpretation\n",
    "   - Recommendations for practical implementation\n",
    "\n",
    "4. **Model Artifacts**\n",
    "   - Trained model weights\n",
    "   - Feature importance rankings\n",
    "   - Performance metrics by segment\n",
    "\n",
    "---\n",
    "\n",
    "## Project Timeline\n",
    "\n",
    "| Phase | Duration | Deliverables |\n",
    "|-------|----------|-------------|\n",
    "| **Data Exploration** | Week 1 | ✅ Complete analysis notebook |\n",
    "| **Data Preprocessing** | Week 2 | Feature engineering pipeline |\n",
    "| **Baseline Models** | Week 3 | Traditional forecasting benchmarks |\n",
    "| **Neural Networks** | Week 4-5 | MLP and LSTM implementations |\n",
    "| **Evaluation & Analysis** | Week 6 | Model comparison and insights |\n",
    "| **Documentation** | Week 7 | Final report and presentation |\n",
    "\n",
    "---\n",
    "\n",
    "*This project aims to provide both theoretical insights into neural network applications in time series forecasting and practical tools for retail demand prediction.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

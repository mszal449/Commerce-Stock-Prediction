{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Clean MLP Implementation for Store Sales Forecasting\n",
    "\n",
    "A properly designed Multi-Layer Perceptron for time series forecasting with correct data handling, feature engineering, and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import wandb\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../src')\n",
    "from evaluation.metrics import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../content/data_processed')\n",
    "RESULTS_DIR = Path('../results/neural_networks/mlp')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handles all data preprocessing operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        self.target_transform_params = {}\n",
    "        \n",
    "    def fit_target_transform(self, target_values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fit and apply target transformation (log1p with shift for negative values)\"\"\"\n",
    "        min_val = target_values.min()\n",
    "        shift = abs(min_val) + 1 if min_val <= 0 else 0\n",
    "        \n",
    "        self.target_transform_params = {\n",
    "            'method': 'log1p_shift',\n",
    "            'shift': shift,\n",
    "            'original_min': min_val,\n",
    "            'original_max': target_values.max(),\n",
    "            'original_mean': target_values.mean()\n",
    "        }\n",
    "        \n",
    "        transformed = np.log1p(target_values + shift)\n",
    "        print(f\"Target transform: shift={shift:.3f}, original_range=[{min_val:.3f}, {target_values.max():.3f}]\")\n",
    "        print(f\"Transformed range: [{transformed.min():.3f}, {transformed.max():.3f}]\")\n",
    "        \n",
    "        return transformed\n",
    "    \n",
    "    def transform_target(self, target_values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply fitted target transformation\"\"\"\n",
    "        shift = self.target_transform_params['shift']\n",
    "        return np.log1p(target_values + shift)\n",
    "    \n",
    "    def inverse_transform_target(self, transformed_values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Inverse target transformation\"\"\"\n",
    "        shift = self.target_transform_params['shift']\n",
    "        return np.expm1(transformed_values) - shift\n",
    "    \n",
    "    def add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add comprehensive time-based features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'date' not in df.columns:\n",
    "            return df\n",
    "            \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Basic time features\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "        df['dayofweek'] = df['date'].dt.dayofweek\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        df['dayofyear'] = df['date'].dt.dayofyear\n",
    "        df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "        \n",
    "        # Binary features\n",
    "        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "        df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "        df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "        df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "        df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "        \n",
    "        # Cyclical features (important for time series)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "        df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "        df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "        \n",
    "        print(f\"Added {len([c for c in df.columns if c not in ['date']])} time features\")\n",
    "        return df\n",
    "    \n",
    "    def fit_categorical_encoders(self, df: pd.DataFrame, categorical_columns: List[str]):\n",
    "        \"\"\"Fit label encoders for categorical variables\"\"\"\n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                encoder = LabelEncoder()\n",
    "                # Add 'unknown' category to handle unseen values\n",
    "                unique_vals = list(df[col].unique()) + ['<UNKNOWN>']\n",
    "                encoder.fit(unique_vals)\n",
    "                self.encoders[col] = encoder\n",
    "                print(f\"Encoder for {col}: {len(encoder.classes_)} classes\")\n",
    "    \n",
    "    def transform_categorical(self, df: pd.DataFrame, categorical_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Transform categorical variables using fitted encoders\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns and col in self.encoders:\n",
    "                # Handle unseen categories\n",
    "                mask = df[col].isin(self.encoders[col].classes_)\n",
    "                df.loc[~mask, col] = '<UNKNOWN>'\n",
    "                df[col] = self.encoders[col].transform(df[col])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_numerical_scalers(self, df: pd.DataFrame, numerical_columns: List[str]):\n",
    "        \"\"\"Fit scalers for numerical features\"\"\"\n",
    "        for col in numerical_columns:\n",
    "            if col in df.columns:\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(df[[col]])\n",
    "                self.scalers[col] = scaler\n",
    "        print(f\"Fitted scalers for {len(self.scalers)} numerical columns\")\n",
    "    \n",
    "    def transform_numerical(self, df: pd.DataFrame, numerical_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Transform numerical features using fitted scalers\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        for col in numerical_columns:\n",
    "            if col in df.columns and col in self.scalers:\n",
    "                df[col] = self.scalers[col].transform(df[[col]]).flatten()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_processors(self, path: Path):\n",
    "        \"\"\"Save all processors for later use\"\"\"\n",
    "        import joblib\n",
    "        \n",
    "        processor_data = {\n",
    "            'scalers': self.scalers,\n",
    "            'encoders': self.encoders,\n",
    "            'target_transform_params': self.target_transform_params\n",
    "        }\n",
    "        \n",
    "        joblib.dump(processor_data, path / 'data_processors.pkl')\n",
    "        print(f\"Saved processors to {path / 'data_processors.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, features: torch.Tensor, targets: torch.Tensor):\n",
    "        assert len(features) == len(targets), \"Features and targets must have same length\"\n",
    "        \n",
    "        self.features = features.float()\n",
    "        self.targets = targets.float()\n",
    "        \n",
    "        assert not torch.isnan(self.features).any(), \"Features contain NaN values\"\n",
    "        assert not torch.isnan(self.targets).any(), \"Targets contain NaN values\"\n",
    "        assert not torch.isinf(self.features).any(), \"Features contain Inf values\"\n",
    "        assert not torch.isinf(self.targets).any(), \"Targets contain Inf values\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "    \n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self.features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):    \n",
    "    def __init__(self, \n",
    "            input_dim: int, \n",
    "            hidden_dims: List[int] = [512, 256, 128], \n",
    "            dropout: float = 0.3, \n",
    "            batch_norm: bool = True,\n",
    "            skip_connections: bool = False\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout = dropout\n",
    "        self.skip_connections = skip_connections\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            # Batch normalization\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(self.activation)\n",
    "            \n",
    "            # Dropout\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        pred_clamped = torch.clamp(predictions, min=0) + self.epsilon\n",
    "        \n",
    "        log_pred = torch.log1p(pred_clamped)\n",
    "        log_true = torch.log1p(targets + self.epsilon)\n",
    "        \n",
    "        mse_log = torch.mean((log_pred - log_true) ** 2)\n",
    "        rmsle = torch.sqrt(mse_log)\n",
    "        \n",
    "        return rmsle\n",
    "    \n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, \n",
    "                 config: dict, device, use_wandb: bool = True):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.use_wandb = use_wandb\n",
    "        \n",
    "        # Initialize WandB\n",
    "        if self.use_wandb:\n",
    "            wandb.init(\n",
    "                project=\"store-sales-forecasting\",\n",
    "                name=f\"mlp-rmsle-{int(time.time())}\",\n",
    "                config=config,\n",
    "                tags=[\"mlp\", \"neural-network\", \"time-series\", \"rmsle-loss\"],\n",
    "                reinit=True\n",
    "            )\n",
    "            print(\"WandB initialized successfully\")\n",
    "        \n",
    "        # Loss and optimizer            \n",
    "        self.criterion = RMSLELoss(epsilon=1e-6)\n",
    "        print(\"Using RMSLE loss function\")\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            betas=(0.9, 0.95)\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (features, targets) in enumerate(self.train_loader):\n",
    "            features, targets = features.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(features)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if self.config.get('gradient_clip', 0) > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['gradient_clip'])\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"Evaluate model on given dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in dataloader:\n",
    "                features, targets = features.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(features)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, np.array(predictions), np.array(actuals)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Model parameters: {self.model.get_num_parameters():,}\")\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_preds, val_actuals = self.evaluate(self.val_loader)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_metrics = summary(val_actuals, val_preds) if val_preds.std() > 1e-8 else {'MAE': float('inf'), 'RMSE': float('inf'), 'RMSLE': float('inf'), 'MAPE': float('inf'), 'SMAPE': float('inf')}\n",
    "            \n",
    "            # Learning rate scheduling based on RMSLE loss\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping and best model saving\n",
    "            if val_loss < self.best_val_loss - self.config['min_delta']:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.patience_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                self.save_checkpoint(epoch, val_metrics)\n",
    "                status = \"Best\"\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                status = f\"{self.patience_counter}/{self.config['patience']}\"\n",
    "            \n",
    "            # Logging\n",
    "            epoch_time = time.time() - start_time\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_rmsle_loss': train_loss,\n",
    "                'val_rmsle_loss': val_loss,\n",
    "                'val_mae': val_metrics['MAE'],\n",
    "                'val_rmse': val_metrics['RMSE'],\n",
    "                'val_rmsle': val_metrics['RMSLE'],\n",
    "                'val_mape': val_metrics['MAPE'],\n",
    "                'val_smape': val_metrics['SMAPE'],\n",
    "                'learning_rate': lr,\n",
    "                'epoch_time': epoch_time,\n",
    "                'patience_counter': self.patience_counter\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d}/{self.config['epochs']} | \"\n",
    "                f\"Train RMSLE: {train_loss:.6f} | Val RMSLE: {val_loss:.6f} | \"\n",
    "                f\"Val RMSLE Metric: {val_metrics['RMSLE']:.6f} | \"\n",
    "                f\"LR: {lr:.2e} | Time: {epoch_time:.1f}s | {status}\")\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.config['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"\\nLoaded best model with validation RMSLE loss: {self.best_val_loss:.6f}\")\n",
    "        \n",
    "        # Finish WandB run\n",
    "        if self.use_wandb:\n",
    "            wandb.finish()\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_metrics):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': self.best_val_loss,\n",
    "            'val_metrics': val_metrics,\n",
    "            'config': self.config,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, RESULTS_DIR / 'best_model.pt')\n",
    "    \n",
    "    def final_evaluation(self):\n",
    "        \"\"\"Final evaluation on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"FINAL EVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Test evaluation\n",
    "        test_loss, test_preds, test_actuals = self.evaluate(self.test_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_metrics = summary(test_actuals, test_preds)\n",
    "        \n",
    "        print(f\"Test RMSLE Loss: {test_loss:.6f}\")\n",
    "        print(\"\\nTest Metrics (original scale):\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"  {metric}: {value:.6f}\")\n",
    "        \n",
    "        return test_metrics, test_preds, test_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model architecture\n",
    "    'hidden_dims': [512, 256, 128, 64],\n",
    "    'dropout': 0.3,\n",
    "    'activation': 'gelu',\n",
    "    'batch_norm': True,\n",
    "    \n",
    "    # Training\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 100,\n",
    "    'patience': 15,\n",
    "    'min_delta': 1e-5,\n",
    "    'gradient_clip': 1.0,\n",
    "    \n",
    "    # Data\n",
    "    'categorical_columns': ['store_nbr', 'family', 'city', 'state', 'type', 'cluster'],\n",
    "    'target_column': 'sales'\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "train_df = pd.read_parquet(DATA_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(DATA_DIR / 'val.parquet')\n",
    "test_df = pd.read_parquet(DATA_DIR / 'test.parquet')\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Validation: {val_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")\n",
    "\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing data for MLP training...\")\n",
    "\n",
    "# Extract features and targets\n",
    "target_col = CONFIG['target_column']\n",
    "exclude_cols = [target_col, 'date'] if 'date' in train_df.columns else [target_col]\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "# Identify which columns were originally categorical (now encoded as integers)\n",
    "categorical_cols = [col for col in CONFIG['categorical_columns'] if col in feature_cols]\n",
    "numerical_cols = [col for col in feature_cols if col not in categorical_cols]\n",
    "\n",
    "print(f\"\\nData preparation:\")\n",
    "print(f\"  Target column: {target_col}\")\n",
    "print(f\"  Total feature columns: {len(feature_cols)}\")\n",
    "print(f\"  Categorical columns (encoded): {len(categorical_cols)} - {categorical_cols}\")\n",
    "print(f\"  Numerical columns (scaled): {len(numerical_cols)}\")\n",
    "print(f\"  Excluded columns: {exclude_cols}\")\n",
    "\n",
    "X_train = train_df[feature_cols].values.astype(np.float32)\n",
    "y_train = train_df[target_col].values.astype(np.float32)\n",
    "\n",
    "X_val = val_df[feature_cols].values.astype(np.float32)\n",
    "y_val = val_df[target_col].values.astype(np.float32)\n",
    "\n",
    "X_test = test_df[feature_cols].values.astype(np.float32)\n",
    "y_test = test_df[target_col].values.astype(np.float32)\n",
    "\n",
    "print(f\"\\nFinal data shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Check categorical value ranges (important for embeddings if used later)\n",
    "print(f\"\\nCategorical variable ranges (for reference):\")\n",
    "for col in categorical_cols:\n",
    "    if col in train_df.columns:\n",
    "        min_val = train_df[col].min()\n",
    "        max_val = train_df[col].max()\n",
    "        unique_count = train_df[col].nunique()\n",
    "        print(f\"  {col}: range=[{min_val}, {max_val}], unique_values={unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "print(\"Creating datasets and data loaders...\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SalesDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = SalesDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = SalesDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "print(f\"  Input dimension: {train_dataset.input_dim}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(f\"\\nTesting data loading...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"  Sample batch shapes: {sample_batch[0].shape}, {sample_batch[1].shape}\")\n",
    "print(f\"  Data loading successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize model\n",
    "print(\"Creating model...\")\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=train_dataset.input_dim,\n",
    "    hidden_dims=CONFIG['hidden_dims'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    batch_norm=CONFIG['batch_norm']\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Parameters: {model.get_num_parameters():,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_input = sample_batch[0][:5].to(device)\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"  Input shape: {sample_input.shape}\")\n",
    "    print(f\"  Output shape: {sample_output.shape}\")\n",
    "    print(f\"  Output range: [{sample_output.min():.3f}, {sample_output.max():.3f}]\")\n",
    "    print(f\"  Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    config=CONFIG,\n",
    "    device=device,\n",
    "    use_wandb=True\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "test_metrics, test_preds, test_actuals = trainer.final_evaluation()\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    'model_type': 'MLP_Preprocessed_Data',\n",
    "    'config': CONFIG,\n",
    "    'model_parameters': model.get_num_parameters(),\n",
    "    'test_metrics': test_metrics,\n",
    "    'data_source': 'preprocessed_parquet_files'\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Training curves\n",
    "axes[0, 0].plot(trainer.train_losses, label='Training RMSLE', alpha=0.8)\n",
    "axes[0, 0].plot(trainer.val_losses, label='Validation RMSLE', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('RMSLE Loss')\n",
    "axes[0, 0].set_title('Training Progress - RMSLE Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions vs Actuals\n",
    "axes[0, 1].scatter(test_actuals, test_preds, alpha=0.6, s=1)\n",
    "min_val, max_val = min(test_actuals.min(), test_preds.min()), max(test_actuals.max(), test_preds.max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Sales')\n",
    "axes[0, 1].set_ylabel('Predicted Sales')\n",
    "axes[0, 1].set_title('Predictions vs Actuals')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals\n",
    "residuals = test_actuals - test_preds\n",
    "axes[1, 0].scatter(test_preds, residuals, alpha=0.6, s=1)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 0].set_xlabel('Predicted Sales')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residuals Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Error distribution\n",
    "axes[1, 1].hist(residuals, bins=50, alpha=0.7, density=True)\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].set_xlabel('Residuals')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Residuals Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Plots saved to {RESULTS_DIR / 'evaluation_plots.png'}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📈 FINAL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: MLP with {model.get_num_parameters():,} parameters\")\n",
    "print(f\"Architecture: {CONFIG['hidden_dims']}\")\n",
    "print(f\"\")\n",
    "print(f\"Test Metrics (Original Scale):\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric:6s}: {value:10.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"Prediction Quality:\")\n",
    "print(f\"  Actual range:    [{test_actuals.min():8.2f}, {test_actuals.max():8.2f}]\")\n",
    "print(f\"  Predicted range: [{test_preds.min():8.2f}, {test_preds.max():8.2f}]\")\n",
    "print(f\"  Mean error:      {residuals.mean():8.2f}\")\n",
    "print(f\"  Std error:       {residuals.std():8.2f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
